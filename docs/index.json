[
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "Introduction This workshop guides you through deploying a full-stack DNA Analysis application on AWS. The application allows users to analyze DNA sequences, manage results, and visualize biological data.\nApplication Architecture Frontend (React + Vite) Framework: React 18 with TypeScript UI Libraries: Material-UI, TailwindCSS, Recharts State Management: React Context API Routing: React Router v6 Form Handling: React Hook Form with Zod validation HTTP Client: Axios Hosting: S3 + CloudFront CDN Backend (Spring Boot) Framework: Spring Boot 3.x Language: Java 17 Database: MySQL 8.0 with Spring Data JPA Security: Spring Security with JWT authentication API: RESTful API with proper error handling Hosting: EC2 instances with Auto Scaling Database (RDS MySQL) Engine: MySQL 8.0.40 Instance: db.t3.micro (scalable) Storage: 20GB gp3 with encryption Backup: Automated backups with 3-7 days retention High Availability: Multi-AZ deployment (optional) AWS Architecture Network Layer VPC (10.0.0.0/16)\r├── Public Subnets (10.0.1.0/24, 10.0.3.0/24)\r│ ├── Internet Gateway\r│ ├── NAT Gateway\r│ └── Application Load Balancer\r│\r└── Private Subnets (10.0.2.0/24, 10.0.4.0/24)\r├── EC2 Instances (Auto Scaling Group)\r├── RDS MySQL (Multi-AZ)\r└── VPC Endpoints (S3, CloudWatch, SSM, Cognito) Application Flow User Browser\r│\r├─── HTTPS ──\u0026gt; CloudFront ──\u0026gt; S3 (Static Frontend)\r│\r└─── HTTPS ──\u0026gt; API Gateway ──\u0026gt; ALB ──\u0026gt; EC2 (Backend API)\r│\r└──\u0026gt; RDS MySQL Security Architecture Internet\r│\r├─── CloudFront (HTTPS only)\r│ └─── S3 Bucket Policy (CloudFront OAI)\r│\r└─── API Gateway (Resource Policy)\r└─── ALB Security Group (Port 80/443)\r└─── EC2 Security Group (Port 8080 from ALB only)\r└─── RDS Security Group (Port 3306 from EC2 only) Key Features 1. User Authentication User registration and login JWT token-based authentication AWS Cognito integration (optional) Session management 2. DNA Analysis Upload and analyze DNA sequences Support multiple file formats Batch processing capability Store analysis results 3. Data Visualization DNA analysis charts Dashboard with metrics Export results in multiple formats 4. User Management User profile management Analysis history Role-based access control Infrastructure as Code CloudFormation Template The infrastructure.yaml template includes:\nNetworking (Lines 1-400)\nVPC with DNS support 2 Public Subnets (Multi-AZ) 2 Private Subnets (Multi-AZ) Internet Gateway NAT Gateway (can be disabled for cost savings) Route Tables VPC Endpoints (S3, CloudWatch, SSM, Cognito) Compute (Lines 400-700)\nLaunch Template with User Data script Auto Scaling Group (1-4 instances) Application Load Balancer Target Group with health checks Scaling Policies (CPU-based) Storage \u0026amp; CDN (Lines 700-900)\nS3 Bucket for Frontend S3 Bucket Policy CloudFront Distribution CloudFront Origin Access Identity Database (Lines 900-1000)\nRDS MySQL Instance DB Subnet Group Automated Backups Encryption at rest Security (Lines 1000-1200)\nSecurity Groups (ALB, EC2, RDS, VPC Endpoints) IAM Roles (EC2, CloudWatch, S3) IAM Instance Profile Cognito User Pool (optional) Secrets Manager (optional) Monitoring (Lines 1200-1393)\nCloudWatch Log Groups CloudWatch Alarms (CPU, Memory) SNS Topic for alerts API Gateway with CORS Cost Optimization 1. VPC Endpoints instead of NAT Gateway Savings: ~$20-25/month\nS3 Gateway Endpoint: FREE Interface Endpoints: $7.20/endpoint/month Total: ~$28/month vs NAT Gateway $32/month + data transfer 2. Instance Sizing Development: t3.micro ($7-10/month) Production: t3.small or t3.medium\n3. RDS Optimization Single-AZ for development Multi-AZ for production Automated backups with appropriate retention 4. CloudFront Caching Reduce requests to S3 Lower latency for users Free tier: 1TB data transfer/month Best Practices Applied 1. Security ✅ Private subnets for EC2 and RDS ✅ Security Groups with least privilege ✅ IAM Roles instead of hardcoded credentials ✅ Encryption at rest and in transit ✅ VPC Endpoints for private connectivity ✅ CloudTrail for audit logging (optional)\n2. High Availability ✅ Multi-AZ deployment ✅ Auto Scaling Group ✅ Application Load Balancer ✅ RDS automated backups ✅ CloudFront global CDN\n3. Monitoring \u0026amp; Logging ✅ CloudWatch Logs for application logs ✅ CloudWatch Alarms for metrics ✅ SNS notifications ✅ Health checks on ALB and ASG\n4. Automation ✅ Infrastructure as Code with CloudFormation ✅ User Data scripts for EC2 initialization ✅ Systemd service for application management ✅ Automated deployments with scripts\nDeployment Steps Preparation (10 minutes)\nInstall AWS CLI Create EC2 Key Pair Configure parameters Deploy Infrastructure (15-20 minutes)\nValidate CloudFormation template Create stack Wait for resources to be created Deploy Backend (20-30 minutes)\nBuild JAR file Upload to S3 Deploy to EC2 Configure database connection Deploy Frontend (10-15 minutes)\nBuild React application Upload to S3 Invalidate CloudFront cache Testing (15-30 minutes)\nTest authentication Test DNA analysis features Verify monitoring Cleanup (5-10 minutes)\nDelete CloudFormation stack Verify all resources deleted Expected Outcomes After completing this workshop, you will have:\n✅ A working full-stack application on AWS ✅ Deep understanding of AWS networking and security ✅ Experience with Infrastructure as Code ✅ Knowledge of cost optimization ✅ Best practices for production deployment\nReference Resources AWS CloudFormation Documentation AWS VPC Best Practices AWS Well-Architected Framework Spring Boot on AWS React Deployment Best Practices "
},
{
	"uri": "https://phuong721.github.io/learning-aws/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Reflection Report – “AWS Mastery #2 – CloudFormation \u0026amp; CDK Workshop” Purpose of the Event The “AWS Mastery #2 – CloudFormation \u0026amp; CDK” workshop was designed to help participants:\nUnderstand the concepts and mindset behind Infrastructure as Code (IaC). Learn how AWS automates and manages infrastructure using CloudFormation and the AWS Cloud Development Kit (CDK). Gain a clearer picture of container technologies including Docker, Amazon ECS, Amazon EKS, and AWS App Runner. Strengthen DevOps thinking through automated deployments, reproducible environments, and scalable infrastructure. Observe hands-on demos to reinforce practical IaC implementation skills. The workshop was extremely valuable for developers, cloud engineers, and DevOps professionals who want to automate and standardize AWS infrastructure.\nSpeakers Bao Huynh Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Both speakers have strong practical experience deploying real-world AWS systems, which made the workshop highly engaging and easy to follow.\nKey Content 1. Infrastructure as Code (IaC) Mindset The workshop began by analyzing why ClickOps (manually clicking on the AWS console) has significant drawbacks:\nHighly error-prone due to manual operations. Difficult to reproduce environments across teams. No unified change tracking or version control. Limited auditability and challenging rollbacks. IaC was introduced as a modern DevOps foundation:\nAutomation: Infrastructure is fully created and updated via code. Reproducibility: Identical environments can be provisioned consistently. Scalability: Infrastructure can be scaled quickly through versioned code. Collaboration: All changes can be tracked, reviewed, and audited via Git. 2. AWS CloudFormation – AWS Native IaC Tool What is CloudFormation? CloudFormation allows teams to define entire infrastructure stacks using YAML or JSON templates, and AWS will automatically build them.\nCloudFormation Template Anatomy Parameters\nInputs used to customize deployments without changing the template. Mappings\nRegion-specific configurations such as AMI IDs. Conditions\nCreate resources only when certain conditions are met (e.g., only create EC2 for production). Resources\nThe core of every template. Defines S3, EC2, IAM roles, VPC components, etc. Outputs\nProvide values for cross-stack referencing or sharing with other teams. Drift Detection Detects whether resources have been modified outside of CloudFormation. Helps ensure that the actual environment matches the defined template. The speakers provided clear visual examples of state mismatch, helping me understand the importance of drift management.\n3. AWS CDK – Infrastructure Using Real Programming Languages CDK was introduced as a higher-level, developer-friendly IaC framework:\nSupports TypeScript, Python, Java, Go, C#/.NET. CDK code is synthesized into CloudFormation templates (cdk synth). Enables abstraction, reuse, and scalable architectures. Core Concepts Constructs L1: Direct 1:1 mapping to CloudFormation resources. L2: Higher-level APIs with recommended default configurations. L3: Prebuilt architecture patterns, providing out-of-the-box solutions. Stack \u0026amp; App Structure Stack: The unit of deployment. App: A collection of multiple stacks. CDK CLI Commands cdk init – Initialize a new project cdk bootstrap – Prepare AWS environment cdk synth – Generate CloudFormation template cdk deploy – Deploy resources cdk destroy – Remove stacks cdk diff – Preview changes cdk drift – Detect configuration drift CDK significantly reduces manual configuration and improves infrastructure maintainability.\n4. Docker \u0026amp; AWS Container Services Docker Fundamentals Containers are lightweight and start much faster than VMs. Dockerfile defines the environment, dependencies, and build steps. Docker images act as blueprints for creating reproducible containers. Amazon ECR A secure, fully managed container registry that supports:\nImage scanning Immutable tags Lifecycle policies IAM-based access control 5. Orchestration with ECS, EKS, and App Runner Amazon ECS A fully managed, AWS-native orchestration service.\nSupported launch types:\nEC2 launch type – More control, good for long-running workloads. Fargate launch type – Serverless compute, no server management. Key components:\nCluster Task Definition Service Amazon EKS Managed Kubernetes for complex or multi-cloud workloads.\nAutomates Kubernetes control plane. Works with EC2, Fargate, or on-prem via Outposts. AWS App Runner A simple service to run web applications or APIs:\nAutomatically builds from GitHub or ECR. No servers to manage. Ideal for small teams and rapid deployments. What I Learned 1. Modern Infrastructure Thinking IaC is more than a tool—it is a standardized operational approach. Drift detection is essential in maintaining infrastructure reliability. Version-controlled infrastructure dramatically reduces risk. 2. Deep Understanding of IaC Tools I can now confidently read and write CloudFormation templates with all major components. CDK’s L2 and L3 constructs help reduce boilerplate and enforce best practices. Understanding the conversion process from CDK → CloudFormation is extremely valuable. 3. Container Architecture Clear distinction of when to use ECS, Fargate, or EKS. Improved ability to evaluate compute models based on cost, performance, and operational needs. 4. Practical DevOps Experience cdk diff is a powerful safety tool before deploying to production. IaC ensures that environments across teams remain consistent and reviewable. I now understand how CI/CD pipelines integrate with IaC and containers. Application to My Work Begin migrating small infrastructure components to IaC using CloudFormation or CDK. Build practice templates for S3, IAM roles, and VPC components. Deploy a sample application using ECS Fargate to understand end-to-end workflows. Use CDK to design reusable infrastructure for internal projects. Recommend using ECR as the standard registry for container pipelines. Create internal documentation on using cdk diff to enforce safe deployments. Event Experience The workshop was well-structured, informative, and highly practical. Live demos gave me a clear understanding of how IaC works in real AWS environments. I particularly enjoyed the comparison between ECS and EKS—very helpful for architectural decisions. Networking with other participants expanded my understanding of real-world DevOps problems. The content has significantly increased my confidence in applying IaC and container orchestration in actual projects. Event Photos Event Photos Overall, the workshop provided not only deep technical knowledge but also shaped my mindset toward modern cloud infrastructure, helping me approach future projects with more clarity and confidence.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.9-week9/",
	"title": "Worklog Week 9",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Collect requirements and start developing content for the Proposal and Proposal Template of the BDSS project. Write introductory sections: Executive Summary, Problem Statement, Solution Overview. Identify the AWS architecture to include in both documents. Complete the first draft of the Proposal and the skeleton structure of the Proposal Template. Tasks to be carried out this week: Day Task Start Date End Date Reference Materials Mon - Receive mentor’s requirements for content to include in the Proposal \u0026amp; Proposal Template - Define the common outline for both documents 03/11/2025 03/11/2025 proposal.docx Proposal Template.docx Tue - Write Executive Summary for the Proposal - Prepare the corresponding introductory section in the Proposal Template 04/11/2025 04/11/2025 proposal.docx Wed - Write Problem Statement: describe issues at healthcare facilities \u0026amp; the need to connect blood donors - Adjust content for inclusion in the Proposal Template 05/11/2025 05/11/2025 Team documents Thu - Write Proposed Solution for the Proposal - Simultaneously complete the Solution Overview section in the Proposal Template 06/11/2025 06/11/2025 proposal.docx Fri - Describe Solution Architecture based on 4 layers - Review the presentation of the architecture in the Proposal Template to match the standard layout 07/11/2025 07/11/2025 AWS Docs Sat - Write Technical Implementation and deployment roadmap - Fill in corresponding sections in the Proposal Template (Activities, Deliverables, Scope…) 08/11/2025 08/11/2025 Proposal Template.docx Sun - Finalize the first draft for both Proposal and Proposal Template - Send to mentor for review and collect feedback - Update Week 9 Worklog 09/11/2025 09/11/2025 Slack/Meeting Week 9 Achievements: Completed the first draft of the Proposal and the skeleton of the Proposal Template, including sections:\nExecutive Summary Problem Statement Proposed Cloud Solution Solution Architecture (4 layers) Technical Implementation (draft) Built a complete structure for the Proposal Template, ready to fill in detailed content next week.\nGained a clearer understanding of mentor requirements and content presentation in two different formats:\nProposal: detailed description Proposal Template: standard AWS model Defined the AWS architecture used in both documents:\nRoute 53, CloudFront API Gateway – EC2 RDS MySQL Cognito Authorization CI/CD Pipeline Monitoring \u0026amp; Security Layer Established a solid content foundation for next week to complete: Technical Plan, Budget Estimate, Risk Assessment, and finalize the Proposal Template.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.8-week8/",
	"title": "Worklog Week 8",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Understand the architectural requirements for the project. Become proficient with draw.io to design system diagrams. Understand the relationships between AWS services in the architecture: Networking, Compute, Database, CI/CD, Monitoring. Complete the Network Architecture Diagram and submit it for mentor review. Tasks for this week: Day Task Start Date Completion Date Reference 2 - Gather architectural requirements from mentors - Define the scope of the diagram (VPC, subnets, routing, frontend, backend, database, CI/CD…) 2025-10-27 2025-10-27 FCJ Internal 3 - Start designing the network architecture diagram in draw.io - Create VPC, public/private subnets, Internet Gateway - Add Route 53, CloudFront, S3 FE bucket 2025-10-28 2025-10-28 AWS Docs 4 - Add API Gateway, EC2, Security Groups - Design RDS in private subnet - Draw the data flow: FE → CloudFront → API → EC2 → RDS 2025-10-29 2025-10-29 AWS Architecture Icons 5 - Integrate CI/CD pipeline (CodePipeline, CodeBuild, CloudFormation) - Add Cognito/Auth and complete the Security Layer 2025-10-30 2025-10-30 aws.amazon.com 6 - Optimize diagram layout, adjust colors and borders - Add numbering for request flows - Export the diagram and send it to mentors for review 2025-10-31 2025-10-31 FCJ Internal 7 - Receive mentor feedback: update subnets, API flow, security layers - Revise the diagram according to suggestions 2025-11-01 2025-11-01 Direct discussion Sun - Summarize lessons learned during the architecture design process - Finalize weekly Worklog 2025-11-02 2025-11-02 Week 8 Achievements: Completed a full AWS Network Architecture Diagram including:\nRoute 53, CloudFront, S3 frontend bucket. API Gateway → EC2 backend. NAT Gateway + Internet Gateway. RDS in private subnet. CI/CD: CodePipeline, CodeBuild, CloudFormation. Monitoring \u0026amp; Security: CloudWatch, CloudTrail, IAM, Secrets Manager, SNS. Clearly understood how the components interact:\nUser request flow for FE/BE. Separation of public and private subnets. NAT mechanism for EC2 to access the internet securely. API Gateway → EC2 → RDS data flow. Learned how to standardize technical diagrams following AWS best practices:\nUse of correct AWS icons and proper service grouping. Layered architecture and numbered processing flows. Finalized the architecture diagram for reporting and mentor evaluation.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.7-week7/",
	"title": "Worklog Week 7",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Deploy and manage data on S3, DynamoDB, and Redshift. Create data pipelines using Kinesis, Glue, DataBrew, and EMR. Analyze data with Athena and Kinesis Data Analytics, and visualize with QuickSight. Build serverless applications and interactive dashboards. Familiarize with CloudShell, AWS SDK, and Cloud9 for programming and data handling tasks. Tasks to be carried out this week: Day Tasks Start Date End Date Resources Mon - Create S3 Bucket, Delivery Stream, sample data, Glue Crawler, data validation, session setup, analysis with Athena, visualization with QuickSight, resource cleanup (Module 07-Lab35-3.1 to 7) 20/10/2025 20/10/2025 https://000035.awsstudygroup.com/ Tue - Explore DynamoDB, console exploration, data backup, cleanup, apply Advanced Design Patterns, deploy global serverless app, event-driven architecture (Module 07-Lab39-1 to 8) 21/10/2025 21/10/2025 https://000039.awsstudygroup.com/ Wed - Prepare and build database, manage table data, cost, tagging, usage, queries, resource cleanup (Module 07-Lab40-2.1 to 4) 22/10/2025 22/10/2025 https://000040.awsstudygroup.com/ Thu - Familiarize with CloudShell, console, SDK; create Cloud9 instance, download \u0026amp; upload dataset to S3; setup DataBrew, data profiling, clean \u0026amp; transform data (Module 07-Lab60 \u0026amp; 07-Lab70) 23/10/2025 23/10/2025 https://000060.awsstudygroup.com/ Fri - Prepare pipeline, ingest \u0026amp; store data, catalog data, transform with Glue (interactive \u0026amp; GUI), DataBrew, EMR; analyze with Athena \u0026amp; Kinesis Data Analytics; visualize with QuickSight; serve with Lambda; warehouse on Redshift (Module 07-Lab72-2 to 13) 24/10/2025 24/10/2025 https://000070.awsstudygroup.com/ Sat - Build dashboard, improve dashboard, create interactive dashboard (Module 07-Lab73-3 to 5) 25/10/2025 25/10/2025 https://000072.awsstudygroup.com/ Sun - Summarize, review results, cleanup resources, self-assess week, and prepare for next week 26/10/2025 26/10/2025 https://000073.awsstudygroup.com/ Achievements in Week 7: Data management \u0026amp; processing:\nDeployed data on S3, DynamoDB, and Redshift. Built data pipelines using Kinesis, Glue, DataBrew, and EMR. Analysis \u0026amp; visualization:\nAnalyzed data using Athena and Kinesis Data Analytics. Visualized data and built interactive dashboards using QuickSight. Serverless applications:\nDeployed serverless applications on DynamoDB and Lambda. Built event-driven architecture for global serverless apps. Programming \u0026amp; data handling tools:\nUtilized CloudShell, AWS SDK, and Cloud9 for data handling and programming tasks. Resource management:\nCleaned up all deployed resources to avoid unexpected costs. Self-assessment:\nGained proficiency in data management, pipeline creation, analysis \u0026amp; visualization, and serverless applications. Prepared for the next week with advanced content on security, tagging, and cost management. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.6-week6/",
	"title": "Worklog Week 6",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Practice deploying VPC, EC2, RDS, and security configurations. Deploy applications and perform backup/restore. Manage EC2 connections via RDP and Fleet Manager. Configure SQL Server and Oracle databases. Perform schema conversion from MSSQL/Oracle to Aurora MySQL. Practice Serverless Migration, checking logs, and troubleshooting test scenarios. Tasks to be performed this week: Day Task Start Date End Date Resources Mon - Create VPC (Module 06-Lab05-2.1) - Create EC2 Security Group (Module 06-Lab05-2.2) - Create RDS Security Group (Module 06-Lab05-2.3) - Create DB Subnet Group (Module 06-Lab05-2.4) - Deploy EC2 instance (Module 06-Lab05-3) - Deploy RDS database instance (Module 06-Lab05-4) - Application Deployment (Module 06-Lab05-5) - Backup \u0026amp; Restore (Module 06-Lab05-6) - Clean up resources (Module 06-Lab05-7) 13/10/2025 13/10/2025 https://000006.awsstudygroup.com/ Tue - Connect EC2 via RDP Client (Module 06-Lab43-01) - Connect EC2 via Fleet Manager (Module 06-Lab43-02) - Configure SQL Server Source DB (Module 06-Lab43-03) 14/10/2025 14/10/2025 https://000043.awsstudygroup.com/ Wed - Connect \u0026amp; configure Oracle Source DB (Module 06-Lab43-04 \u0026amp; 05) - Drop Constraint (Module 06-Lab43-06) - MSSQL → Aurora MySQL target config (Module 06-Lab43-07) - Create migration project (Module 06-Lab43-08) 15/10/2025 15/10/2025 https://000043.awsstudygroup.com/ Thu - Schema conversion MSSQL/Oracle → Aurora MySQL (Module 06-Lab43-09 \u0026amp; 10) - Create Migration Task \u0026amp; Endpoints (Module 06-Lab43-11) - Inspect S3 data (Module 06-Lab43-12) 16/10/2025 16/10/2025 https://000043.awsstudygroup.com/ Fri - Create Serverless Migration (Module 06-Lab43-13) - Create Event Notification (Module 06-Lab43-14) - Check logs (Module 06-Lab43-15) - Troubleshoot Mem Pressure \u0026amp; Table Errors (Module 06-Lab43-16 \u0026amp; 17) 17/10/2025 17/10/2025 https://000043.awsstudygroup.com/ Sat - Consolidate results, review and clean up all resources 18/10/2025 18/10/2025 N/A Sun - Weekly self-assessment and preparation for next week 19/10/2025 19/10/2025 N/A Week 6 Outcomes: Network \u0026amp; Database Deployment:\nCreated and configured VPC, EC2 Security Group, RDS Security Group, DB Subnet Group. Successfully deployed EC2 \u0026amp; RDS instances, application deployment, and performed backup/restore. EC2 Connection Management:\nConnected EC2 via RDP Client and Fleet Manager, managed SQL Server \u0026amp; Oracle Source DB. Data Migration \u0026amp; Conversion:\nPerformed schema conversion from MSSQL/Oracle to Aurora MySQL. Created Migration Task, Endpoints, inspected data on S3. Serverless Migration \u0026amp; Troubleshooting:\nCreated Serverless Migration, configured Event Notification, checked logs. Successfully handled Mem Pressure \u0026amp; Table Errors test scenarios. Resource Management:\nCleaned up all deployed resources to avoid unexpected costs. Self-Assessment:\nMastered deploying VPC, EC2, RDS, backup/restore, database connection, and Serverless Migration. Understood data conversion process and log monitoring. Prepared well for the next week with advanced security and infrastructure optimization topics. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.5-week5/",
	"title": "Worklog Week 5",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Practice AWS Security Hub and security assessment. Manage VPC, EC2, and Lambda with Web-hooks and Tagging. Manage IAM Users, Policies, Roles, and advanced Switch Roles. Practice controlling IAM user permissions and access restrictions. Manage CloudTrail, Athena, and encrypted S3 data. Deploy and manage EC2, S3, and IAM Role/Key. Tasks to be completed this week: Day Task Start Date End Date Reference Mon - Enable AWS Security Hub (Module 05-Lab18-02) - Score for each criteria set (Module 05-Lab18-03) - Clean up Security Hub resources (Module 05-Lab18-04) 06/10/2025 06/10/2025 https://000018.awsstudygroup.com/ Tue - Create VPC, Security Group, EC2 (Module 05-Lab22-2.1 to 2.3) - Configure Incoming Web-hooks Slack (Module 05-Lab22-2.4) - Create Tag for Instance (Module 05-Lab22-3) - Create Role for Lambda (Module 05-Lab22-4) - Stop/Start functions \u0026amp; check result (Module 05-Lab22-5.1 to 6) - Clean up resources (Module 05-Lab22-7) 07/10/2025 07/10/2025 https://000022.awsstudygroup.com/ Wed - Manage Tags on EC2 \u0026amp; AWS Resources (Module 05-Lab27-2.1.1 to 2.2) - Create Resource Group (Module 05-Lab27-3) - Clean up resources (Module 05-Lab27-4) 08/10/2025 08/10/2025 https://000027.awsstudygroup.com/ Thu - Create IAM Users, Policies, Roles (Module 05-Lab28-2.1 to 4) - Switch Roles \u0026amp; access EC2 in multiple Regions (Module 05-Lab28-5.1 to 5.2.5) - Clean up resources (Module 05-Lab28-6) 09/10/2025 09/10/2025 https://000028.awsstudygroup.com/ Fri - Create IAM Limited User \u0026amp; Restriction Policy, test limits (Module 05-Lab30-3 to 5) - Clean up resources (Module 05-Lab30-6) 10/10/2025 10/10/2025 https://000030.awsstudygroup.com/ Sat - Create Policy, Role, Group, User, KMS, S3 Bucket, upload data, CloudTrail, Athena, test encrypted data (Module 05-Lab33-2.1 to 6) - Clean up resources (Module 05-Lab33-7) 11/10/2025 11/10/2025 https://000033.awsstudygroup.com/ Sun - Manage IAM Group, User, Admin Role, Switch Role, restrict by IP \u0026amp; Time (Module 05-Lab44) - Create EC2, S3, IAM Role, Access Key and clean up resources (Module 05-Lab48) 12/10/2025 12/10/2025 https://000044.awsstudygroup.com/ https://000048.awsstudygroup.com/ Week 5 Results: AWS Security Hub:\nEnabled Security Hub, scored based on criteria, cleaned up resources. VPC, EC2 \u0026amp; Lambda:\nCreated VPC, Security Group, EC2, Lambda functions with stop/start, configured Incoming Web-hooks Slack. Used Tags to manage resources, created Resource Group, verified results. IAM Management:\nCreated Users, Policies, Roles, advanced Switch Roles. Deployed IAM Limited User with access restrictions and IP/Time limits. Managed multiple Regions via EC2 console, verified Tag \u0026amp; Policy. CloudTrail, Athena \u0026amp; S3:\nCreated Bucket, uploaded data, enabled CloudTrail, queried data with Athena, tested encrypted data. Resource Management:\nCleaned up all deployed resources after practice to avoid costs. Self-evaluation:\nMastered Security Hub, VPC, EC2, Lambda, Tag, IAM, CloudTrail \u0026amp; Athena operations. Understood and implemented advanced access restrictions for IAM Users. Ready for next week with security and cost optimization topics. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.4-week4/",
	"title": "Worklog Week 4",
	"tags": [],
	"description": "",
	"content": "Objectives for Week 4: Advanced AWS Backup deployment and notification setup. Deploy and manage virtual machines from On-Premises to AWS using AMI. Advanced S3, Storage Gateway, and File Shares management. Deploy Multi-AZ File System with SSD \u0026amp; HDD, monitor performance, manage users. Manage static S3 websites, CloudFront, versioning, replication, and resource cleanup. Tasks to accomplish this week: Day Task Start Date End Date Resources Mon - Create S3 Bucket (Module 04-Lab13-02.1) - Deploy Backup infrastructure (Module 04-Lab13-02.2) - Create Backup Plan (Module 04-Lab13-03) - Set up notifications (Module 04-Lab13-04) 29/09/2025 29/09/2025 https://000013.awsstudygroup.com/, https://000014.awsstudygroup.com/ Tue - Test Restore (Module 04-Lab13-05) - Clean up Backup resources (Module 04-Lab13-06) 30/09/2025 30/09/2025 https://000013.awsstudygroup.com/, https://000014.awsstudygroup.com/ Wed - Work with VMWare Workstation (Module 04-Lab14-01) - Export VM from On-Premises (Module 04-Lab14-02.1) - Upload VM to AWS (Module 04-Lab14-02.2) - Import VM to AWS (Module 04-Lab14-02.3) - Deploy Instance from AMI (Module 04-Lab14-02.4) 01/10/2025 01/10/2025 https://000024.awsstudygroup.com/, https://000025.awsstudygroup.com/ Thu - Manage S3 Bucket ACL (Module 04-Lab14-03.1) - Export VM from Instance (Module 04-Lab14-03.2) - Clean up VM \u0026amp; AWS resources (Module 04-Lab14-05) 02/10/2025 02/10/2025 https://000024.awsstudygroup.com/, https://000025.awsstudygroup.com/ Fri - Create Storage Gateway (Module 04-Lab24-2.1) - Create File Shares (Module 04-Lab24-2.2) - Mount File Shares on On-Premises machine (Module 04-Lab24-2.3) - Clean up resources (Module 04-Lab24-3) 03/10/2025 03/10/2025 https://000024.awsstudygroup.com/ Sat - Create SSD \u0026amp; HDD Multi-AZ File System (Module 04-Lab25-2.2 \u0026amp; 2.3) - Create file shares, test \u0026amp; monitor performance (Module 04-Lab25-3, 4, 5) - Enable deduplication, shadow copies, manage user sessions \u0026amp; quotas, scale throughput \u0026amp; storage (Module 04-Lab25-6 to 12) 04/10/2025 04/10/2025 https://000024.awsstudygroup.com/ Sun - Clean up Multi-AZ environment (Module 04-Lab25-13) - Create S3 Bucket, load data, static website, CloudFront, versioning \u0026amp; replication, test \u0026amp; cleanup (Module 04-Lab57-2.1 to 11) 05/10/2025 05/10/2025 https://000057.awsstudygroup.com/ Achievements for Week 4: Advanced AWS Backup:\nSuccessfully deployed Backup Plan, configured notifications, and tested restore. Cleaned up backup resources. Virtual Machines \u0026amp; AMI:\nExported VM from On-Premises, uploaded \u0026amp; imported to AWS. Deployed Instance from AMI and managed S3 Bucket ACL. Cleaned up VM \u0026amp; resources after testing. Storage Gateway \u0026amp; Multi-AZ File System:\nCreated Storage Gateway, File Shares, and mounted on On-Premises machine. Deployed SSD \u0026amp; HDD Multi-AZ File System, created file shares, tested \u0026amp; monitored performance. Managed users, enabled deduplication, shadow copies, quotas, scaled throughput \u0026amp; storage. S3 \u0026amp; CloudFront:\nCreated S3 Bucket, loaded data, deployed static website, configured CloudFront. Used versioning, replication multi-region, tested \u0026amp; cleaned up resources. Self-evaluation:\nConfident in deploying advanced Backup, Storage Gateway, Multi-AZ File System, VM management \u0026amp; static website. Practiced proper resource cleanup to avoid costs. Ready for next week’s lessons on security, IAM \u0026amp; cost optimization. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.3-week3/",
	"title": "Worklog Week 3",
	"tags": [],
	"description": "",
	"content": "Week 3 Goals: Practice deploying AWS Backup and ensure data restore. Deploy S3, Storage Gateway, manage data and file shares. Set up and manage a static website on S3 with CloudFront. Learn advanced S3 features: versioning, replication, and access management. Clean up resources after testing to avoid unnecessary costs. Tasks for this week: Day Task Start Date End Date Resources Mon - Deploy AWS Backup to the system - Introduction (Module 03-Lab13-01) - Deploy infrastructure (Module 03-Lab13-02.2) - Create Backup Plan (Module 03-Lab13-03) 22/09/2025 22/09/2025 https://000013.awsstudygroup.com/ Tue - Test Restore (Module 03-Lab13-05) - Clean up Backup resources (Module 03-Lab13-06) 23/09/2025 23/09/2025 https://000013.awsstudygroup.com/ Wed - Create S3 Bucket \u0026amp; EC2 for Storage Gateway (Module 03-Lab24-01.1 \u0026amp; 01.2) - Create Storage Gateway and File Shares (Module 03-Lab24-02.1 \u0026amp; 02.2) 24/09/2025 24/09/2025 https://000024.awsstudygroup.com/ Thu - Create S3 Bucket, load data, enable static website (Module 03-Lab57-02.1, 02.2 \u0026amp; 03) - Configure public access and test website (Module 03-Lab57-04, 05, 06) 25/09/2025 25/09/2025 https://000057.awsstudygroup.com/ Fri - Block all public access, configure CloudFront \u0026amp; test (Module 03-Lab57-07.1 to 07.3) - Use bucket versioning, move objects, multi-region replication (Module 03-Lab57-08, 09, 10) 26/09/2025 26/09/2025 https://000057.awsstudygroup.com/ Sat - Clean up S3 \u0026amp; CloudFront resources (Module 03-Lab57-11) 27/09/2025 27/09/2025 https://000057.awsstudygroup.com/ Sun - Weekly summary, evaluate results, record lessons learned on Backup, Storage Gateway, and S3/CloudFront 28/09/2025 28/09/2025 N/A Week 3 Achievements: AWS Backup:\nSuccessfully deployed Backup Plan and tested data restore. Cleaned up backup resources after testing. Storage Gateway \u0026amp; S3:\nCreated S3 Bucket and EC2 for Storage Gateway. Configured Storage Gateway and file shares, loaded data successfully. Static Website \u0026amp; CloudFront:\nDeployed static website on S3, configured public access, and verified display. Configured CloudFront to distribute content, checked operation. Applied versioning, moved objects, and performed multi-region replication. Self-evaluation:\nMastered AWS Backup, Storage Gateway, S3, and CloudFront deployment steps. Practiced restore testing, versioning, replication, and resource cleanup. Ready for next week with advanced AWS exercises on security and cost optimization. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.2-week2/",
	"title": "Worklog Week 2",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Learn and practice AWS network setup using VPC, Subnet, Route Table, Internet Gateway, NAT Gateway, and security mechanisms. Configure EC2 instances in subnets and test connectivity. Set up Hybrid DNS with Route 53 Resolver. Explore and deploy VPC Peering and AWS Transit Gateway. Tasks to implement this week: Day Task Start Date End Date Reference Mon - Introduction to Amazon VPC and AWS Site-to-Site VPN (Module 02-Lab03-01) - Subnets (Module 02-Lab03-01.1) - Route Table (Module 02-Lab03-01.2) - Internet Gateway (IGW) (Module 02-Lab03-01.3) - NAT Gateway (Module 02-Lab03-01.4) 15/09/2025 15/09/2025 https://000003.awsstudygroup.com/ Tue - Configure Security Group (Module 02-Lab03-02.1) - Network ACLs (Module 02-Lab03-02.2) - VPC Resource Map (Module 02-Lab03-02.3) 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/ Wed - Create VPC (Module 02-Lab03-03.1) - Create Subnet (Module 02-Lab03-03.2) - Create Internet Gateway (Module 02-Lab03-03.3) - Create Route Table for Outbound Internet Routing via IGW (Module 02-Lab03-03.4) - Create Security Groups (Module 02-Lab03-03.5) 17/09/2025 17/09/2025 https://000010.awsstudygroup.com/ Thu - Create EC2 Instances in Subnets (Module 02-Lab03-04.1) - Test connection (Module 02-Lab03-04.2) - Create NAT Gateway (Module 02-Lab03-04.3) - EC2 Instance Connect Endpoint (Module 02-Lab03-04.5) 18/09/2025 18/09/2025 https://000010.awsstudygroup.com/ Fri - Set up Hybrid DNS with Route 53 Resolver (Module 02-Lab10-01) - Generate Key Pair (Module 02-Lab10-02.1) - Initialize CloudFormation Template (Module 02-Lab10-02.2) - Configure Security Group (Module 02-Lab10-02.3) - Connect to RDGW (Module 02-Lab10-03) 19/09/2025 19/09/2025 https://000019.awsstudygroup.com/ Sat - DNS setup: Route 53 Outbound Endpoint (Module 02-Lab10-05.1) - Create Resolver Rules (Module 02-Lab10-05.2) - Create Inbound Endpoints (Module 02-Lab10-05.3) - Test results (Module 02-Lab10-05.4) - Clean up resources (Module 02-Lab10-06) 20/09/2025 20/09/2025 https://000019.awsstudygroup.com/ Sun - VPC Peering setup: Introduction (Module 02-Lab19-01) - Initialize CloudFormation Templates (Module 02-Lab19-02.1) - Create Security Group (Module 02-Lab19-02.2) - Create EC2 instance (Module 02-Lab19-02.3) - Update Network ACLs (Module 02-Lab19-03) - Create peering connection (Module 02-Lab19-04) - Configure Route Tables (Module 02-Lab19-05) - Enable Cross-Peer DNS (Module 02-Lab19-06) - Clean up resources (Module 02-Lab19-07) - AWS Transit Gateway setup: Introduction (Module 02-Lab20-01) - Preparation steps (Module 02-Lab20-02) - Create Transit Gateway (Module 02-Lab20-03) - Create TGW Attachments (Module 02-Lab20-04) - Create TGW Route Tables (Module 02-Lab20-05) - Add TGW Routes to VPC Route Tables (Module 02-Lab20-06) - Clean up resources (Module 02-Lab20-07) 21/09/2025 21/09/2025 https://000020.awsstudygroup.com/ Week 2 Results: AWS Networking:\nCreated and configured VPC, Subnet, Route Table, Internet Gateway, NAT Gateway, Security Groups. Deployed EC2 instances in subnets and verified connectivity. Configured EC2 Instance Connect Endpoint for easier access. Hybrid DNS:\nCreated Key Pairs and initialized CloudFormation Templates. Configured Security Groups and connected to RDGW. Created Route 53 Outbound/Inbound Endpoints, set up Resolver Rules, and verified results. Cleaned up DNS resources after testing. VPC Peering \u0026amp; Transit Gateway:\nEstablished VPC Peering, configured Route Tables, and enabled Cross-Peer DNS. Created AWS Transit Gateway, attachments, route tables, and added routes to VPC route tables. Cleaned up resources to avoid unexpected costs. Self-assessment:\nGained hands-on experience with AWS networking, Hybrid DNS, VPC Peering, and Transit Gateway. Successfully deployed, tested, and cleaned up resources. Ready for the following weeks with more advanced knowledge. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://phuong721.github.io/learning-aws/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://phuong721.github.io/learning-aws/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://phuong721.github.io/learning-aws/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Do Doan Duy Phuong\nPhone Number: 0983394370\nEmail: phuongdddse180235@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh City\nMajor: Information Security\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 30/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/",
	"title": "Worklog Summary",
	"tags": [],
	"description": "",
	"content": "Overview: This section introduces your worklog. It summarizes the tasks completed, the number of weeks, and the activities performed.\nI completed the internship program over 12 weeks, focusing on learning and practicing core AWS services, data management, security, serverless applications, data analysis, and visualization. Below is a summary of weekly tasks:\nWeek 1: Getting started with AWS, creating a Free Tier account, security setup, IAM management, basic Budget creation\nWeek 2: VPC setup, subnets, Internet Gateway, NAT Gateway, Security Group, EC2 instances, site-to-site VPN, Route53 hybrid DNS, VPC peering, Transit Gateway\nWeek 3: AWS Backup deployment, creating backup plans, restore testing; S3 Bucket, Storage Gateway, static website deployment with S3 \u0026amp; CloudFront, versioning, replication\nWeek 4: S3 bucket creation, infrastructure deployment, backup plan \u0026amp; notifications, restore test; VM import/export, Storage Gateway, Multi-AZ file system, performance testing, advanced S3 website\nWeek 5: Security Hub, managing VPC, EC2 \u0026amp; Lambda with tagging; IAM Users, Policies, Roles, KMS; CloudTrail \u0026amp; Athena; permission management; resource cleanup\nWeek 6: VPC, EC2 \u0026amp; RDS Security Groups, RDS database deployment, application deployment, backup/restore; EC2 connection via RDP \u0026amp; Fleet Manager; SQL Server \u0026amp; Oracle configuration; migration tasks; troubleshooting\nWeek 7: Data management on S3, DynamoDB, Redshift; data pipelines with Kinesis, Glue, DataBrew, EMR; data analysis with Athena \u0026amp; Kinesis Data Analytics; visualization with QuickSight; serverless applications and interactive dashboards; CloudShell, SDK, Cloud9 usage\nWeek 8: Building AWS architecture for the project; collecting requirements, creating Network Architecture Diagram, designing VPC, subnets, routing, EC2, RDS, API Gateway, CloudFront, CI/CD, and finalizing the diagram based on mentor feedback\nWeek 9: Drafting the BDSS Proposal; writing Executive Summary, Problem Statement, Solution Overview; analyzing BDSS architecture; consolidating content from team proposals and standardizing according to the template\nWeek 10: Building a workshop based on the Proposal \u0026amp; Proposal Template; writing Introduction, Background, Objectives; creating Architecture, Data Flow, CI/CD, Security; consolidating Cost \u0026amp; Risk to complete the Workshop Draft\nWeek 11: Consolidating content from 2 proposals; standardizing Executive Summary, Problem, Solution Architecture; building slide outline; converting all content to English for final report preparation\nWeek 12: Presenting the project to mentors; collecting feedback on architecture, CI/CD, cost, security; revising slides and documents; compiling action items for final report submission\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console Select the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor Navigate to the VPC menu by using the Search box at the top of the browser window. Click on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes. Click Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.1-week1/",
	"title": "Worklog Week 1",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Set up and manage a basic AWS account. Get familiar with security mechanisms, user management, budgeting, and support on AWS. Practice operations on AWS Console and AWS CLI. Tasks to implement this week: Day Task Start Date End Date Reference Mon - Create AWS Free Tier account (Module 01-Lab01-01) - Set up Virtual MFA (Module 01-Lab01-02) 08/09/2025 08/09/2025 https://000001.awsstudygroup.com/ Tue - Create Admin Group and Admin User (Module 01-Lab01-03) - Configure Account Authentication Support (Module 01-Lab01-04) 09/09/2025 09/09/2025 https://000007.awsstudygroup.com/ Wed - Create Budget using template (Module 01-Lab07-01) - Create Cost Budget (Module 01-Lab07-02) 10/09/2025 10/09/2025 https://000009.awsstudygroup.com/ Thu - Create Usage Budget (Module 01-Lab07-03) - Create Reservation Instance (RI) Budget (Module 01-Lab07-04) 11/09/2025 11/09/2025 https://000009.awsstudygroup.com/ Fri - Create Savings Plans Budget (Module 01-Lab07-05) - Clean Up Budgets (Module 01-Lab07-06) 12/09/2025 12/09/2025 https://000009.awsstudygroup.com/ Sat - Explore AWS Support plans (Module 01-Lab09-01) - Types of support requests (Module 01-Lab09-02) - Change support plan (Module 01-Lab09-03) - Manage support requests (Module 01-Lab09-04) 13/09/2025 13/09/2025 https://000007.awsstudygroup.com/ Sun - Review, consolidate, and finalize configurations; clean up experimental resources 14/09/2025 14/09/2025 - Week 1 Results: AWS Account Initialization:\nSuccessfully created AWS Free Tier account. Set up MFA for root account for security. Created Admin Group and Admin User for management instead of using root. Completed Account Authentication Support configuration. Budget Management:\nCreated Budget using template, Cost Budget, Usage Budget, RI Budget, and Savings Plans Budget. Practiced cleaning up created budgets to avoid extra charges. Support and Troubleshooting:\nUnderstood and learned to use AWS Support plans. Practiced creating, managing, and changing support requests on AWS Console. Self-assessment:\nMastered the process of account setup, security, and user/group management. Practiced basic AWS budgeting and support plan management. Ready to proceed to the following weeks with more advanced AWS services. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Accelerate Your Cloud Strategy with Megaport’s 25 Gbps Hosted AWS Direct Connect As businesses move critical workloads to the cloud, network performance has become a fundamental business requirement. Amazon Web Services (AWS) Direct Connect provides a dedicated network connection between on-premises data centers and AWS. This bypasses the public internet to deliver more stable and reliable network performance with lower latency. The introduction of 25 Gbps hosted connections fills the gap between 10 Gbps options (often insufficient) and 100 Gbps options (often excessive), allowing organizations to right-size their connections without sacrificing performance. Megaport, a leading Network-as-a-Service (NaaS) provider and AWS Marketplace Partner, is among the first to offer this 25 Gbps connection across multiple Direct Connect Edge Locations through its Global Software-Defined Network, spanning hundreds of data centers worldwide. For the latest information, please refer to the Megaport public network footprint page.\nUsing Megaport’s self-service platform, organizations can provision, scale, and manage high-performance AWS connections in minutes rather than weeks or months. In this article, we describe how the powerful combination of AWS and Megaport services enables cloud architects and IT leaders to build next-generation hybrid networks that support data-intensive applications, enhance security, and optimize costs—while maintaining the flexibility to adapt to evolving business needs.\nPrerequisites We assume you are familiar with core networking structures on AWS, particularly Direct Connect. While we won’t dive deeply into definitions, we will highlight its role in supporting hybrid connection architectures involving hosted Direct Connect connections. If you are not familiar with these concepts, we recommend reviewing the Direct Connect documentation for more details on choosing between Direct Connect dedicated and hosted connections.\nFor foundational knowledge, the Getting Started with AWS Direct Connect guide is also a helpful resource.\nKey Use Cases for 25 Gbps Direct Connect The following sections outline key use cases for Direct Connect 25 Gbps.\n1. Cloud Migration Large-scale enterprise data migrations involve transferring massive datasets. A 25 Gbps hosted connection allows organizations to reduce migration time from days to hours while maintaining stable and secure performance. For example, migrating a 100 TB database, which would take over 22 hours with a 10 Gbps connection, can be completed in approximately 9 hours, significantly shortening time-to-production.\nConnection Speed Data Volume Estimated Migration Time Performance Gain 10 Gbps 100 TB ~22 hours Baseline 25 Gbps 100 TB ~9 hours ~59% faster Table 1: Bandwidth effects on large-scale data migration\n2. Data Transfer Organizations collecting data from edge locations or on-premises environments benefit from high-throughput, predictable connections for analytics, backup, and storage workloads. Media companies can efficiently transfer large video files between studios and AWS, while healthcare organizations can move imaging datasets to the cloud for AI analysis while maintaining compliance through private connectivity. The 25 Gbps tier is particularly useful for IoT deployments generating high volumes of sensor data, large-scale backups with strict Recovery Point Objective (RPO) requirements, and organizations training ML models with substantial on-premises datasets.\n3. Hybrid Cloud Support As hybrid architectures become more prevalent, reliable connectivity between on-premises systems and AWS is essential. Megaport’s 25 Gbps hosted connection provides the capacity needed for distributed databases, hybrid storage solutions, and microservices spanning multiple environments. Organizations can implement consistent security policies and seamless application experiences across their entire technology stack, while maintaining performance headroom for peak workloads and future growth.\n4. Latency-Sensitive Applications Applications such as financial trading platforms, automation systems, and real-time collaboration tools require minimal latency. The 25 Gbps dedicated connection maintains stable, low-latency performance by bypassing the public internet while providing sufficient bandwidth to prevent congestion during peak periods. For industries requiring millisecond-level speed—such as high-frequency trading, online gaming, or telehealth—Direct Connect’s predictable performance offers competitive advantages while maintaining security through private connectivity.\nIndustries benefiting include: high-frequency trading, online gaming, telehealth.\n5. Cost Control The 25 Gbps package offers a cost-effective solution to expand network capacity without over-provisioning. Organizations that previously had to choose between insufficient 10 Gbps connections or excessive 100 Gbps connections can now select an optimal balance, often saving 50–60% compared to the 100 Gbps alternative. Megaport’s flexible platform also allows businesses to adjust bandwidth as demands change, supporting cost optimization across the application lifecycle while ensuring required performance for modern cloud workloads.\nBenefits of Deploying AWS Direct Connect 25 Gbps with Megaport The following sections highlight the benefits of deploying Direct Connect 25 Gbps with Megaport.\n1. Private, Secure Data Transfer Megaport’s hosted connections provide a private path to AWS, bypassing the public internet, enhancing data privacy, reducing exposure to common security threats, and ensuring compliance for sensitive workloads. Private connectivity is increasingly critical as regulations like GDPR, HIPAA, and industry-specific requirements impose stricter controls on data movement and protection. The 25 Gbps tier delivers this security without compromising the performance needed for modern data-intensive applications.\n2. Ease of Use Megaport’s self-service portal enables customers to provision, scale, and manage Direct Connect connections in minutes. This flexibility allows teams to quickly adapt to changing project demands without incurring manual management costs. Organizations can establish connections to AWS Regions almost in real-time, rather than waiting weeks or months for traditional telecom circuits, accelerating cloud initiatives and reducing time-to-value for new projects.\n3. Global Reach Megaport’s presence in over 975 data centers across 26+ countries provides near-ubiquitous access to Direct Connect locations. Organizations with distributed footprints can standardize a consistent connection approach across AWS Regions, simplifying architecture and operations while maintaining high performance. This global reach is particularly valuable for multinational businesses operating across time zones or organizations with strict data sovereignty requirements.\n4. Flexibility and Scalability As demand increases, customers can flexibly adjust bandwidth via the Megaport portal. This flexibility allows IT teams to scale cost-effectively while maintaining optimal performance for critical workloads. The 25 Gbps tier offers an ideal intermediate solution, deployable as a long-term option or as a stepping stone for a broader cloud network strategy.\nGetting Started with AWS and Megaport 25 Gbps The following sections guide you on how to use AWS and Megaport with a 25 Gbps hosted connection.\nPrerequisites An active Megaport account with billing enabled. An AWS account with Direct Connect access. Step 1: Create a Megaport Cloud Router (MCR) Log in to the Megaport Portal. Select +Add Service and choose MCR (Megaport Cloud Router) as shown in Figure 1. Ensure the MCR supports at least 25 Gbps capacity, as this determines the maximum speed available for your hosted connection. Follow the on-screen instructions to complete setup. More details can be found in the Creating MCR Documentation. Step 2: Create a Hosted Connection from Megaport to AWS In the Megaport Portal, select your MCR and click + Add Connection. Choose AWS Direct Connect from the Cloud options. Provide the necessary information and follow the instructions: Creating a Hosted Connection Connection Name: a descriptive name for your connection. Service Level Reference: provide a unique identifier for billing or tracking purposes. Rate Limit: set to 25,000 Mbps to provision a 25 Gbps connection. Submit and deploy the connection. You can also create hosted VIFs and other connection types (e.g., public VIF, transit VIF), depending on your use case.\nStep 3: Accept the Hosted Connection in AWS Log in to the AWS Console and navigate to AWS Direct Connect. In the navigation pane, select Connections. Select the hosted connection and choose View details. Select the confirmation checkbox and click Accept. Step 4: Create a Virtual Interface for the Hosted Connection After accepting the connection, select it and choose Create Virtual Interface as shown in Figure 3. Choose the interface type – typically Private to access a VPC. Configure as follows: Virtual interface name VLAN ID (must match the VLAN used in Megaport configuration) BGP ASN (your default or AWS-provided) BGP peer IP address (AWS provides one side; you specify your side) Gateway association: select a Virtual Private Gateway or AWS Transit Gateway attached to your VPC. Click Create. Step 5: Configure BGP on the Megaport MCR Return to the Megaport Portal. Edit the Virtual Cross Connect (VXC) to match the BGP details provided by AWS. Enter the required information: BGP peer IP addresses (yours and AWS’s) Your ASN (or AWS-assigned ASN) Save and apply the configuration. Conclusion In this article, we explored how AWS Direct Connect 25 Gbps hosted connections via Megaport can transform your cloud connectivity strategy. We covered key use cases such as large-scale cloud migrations, data-intensive applications, hybrid cloud deployments, and latency-sensitive workloads. You learned how this solution provides private, secure data transfer with flexible scalability, while offering significant cost savings compared to 100 Gbps alternatives. We also provided step-by-step guidance for setting up a 25 Gbps hosted connection using Megaport’s self-service platform.\nCall to Action Assess your current and future network throughput requirements. Explore 25 Gbps hosted connection options on the AWS Direct Connect Partners page. Visit Megaport’s AWS solution page to learn more or start self-service provisioning. About the Authors Mokshith Kumar\nSenior GTM Solutions Architect – Core Networking at AWS, supporting ISVs and FSI in North America.\nRole: Develop GTM strategy, lead strategic initiatives, drive AWS networking adoption. Interests: Swimming, music. Miranda Li\nSenior Solutions Architect at AWS, specializing in ISVs and cloud-native architecture.\n4 years of experience supporting ISV innovation and scaling on AWS. Expertise: IaaS, networking architecture, security, data analytics. Interests: Badminton, running, outdoor activities. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Reflection Report – “AWS Cloud Club – First Cloud AI Journey Workshop” Purpose of the Event The “AWS Cloud Club – First Cloud AI Journey” workshop was organized to:\nProvide orientation for students beginning their journey into cloud computing and AI. Introduce the AWS Cloud Clubs community and its role in fostering cloud knowledge. Equip participants with foundational knowledge to prepare for deeper cloud/AI learning tracks. Connect tech–driven students with the AWS community in Vietnam. Create a collaborative environment where learners can exchange knowledge and gain insights from others in the same field. Speakers Le Vu Xuan An – AWS Cloud Club Captain HCMUTE Tran Duc Anh – AWS Cloud Club Captain SGU Tran Doan Cong Ly – AWS Cloud Captain PTIT Danh Hoang Hieu Nghi – AWS CLoud Captain HUFLIT Key Content Covered 1. Introduction to AWS Cloud Clubs AWS Cloud Clubs are student-led communities supported by AWS. Each university has a Cloud Club Captain responsible for leading academic and technical activities. The community aims to: Enable peer-to-peer learning. Provide guidance on preparing for AWS certifications. Organize workshops ranging from fundamentals to advanced topics. 2. First Cloud AI Journey – Learning Roadmap The speakers introduced an overview of the First Cloud AI Journey program, including:\nCloud learning path: AWS Cloud fundamentals → hands-on labs → AI/ML basics → real-world projects Foundations of: Cloud computing (EC2, S3, Lambda…) Basic AI concepts Key elements for GenAI (data, vector databases, prompt engineering…) Introduction to AWS services supporting AI learning (to be covered in later sessions). 3. Cloud Learning Experiences Shared by Captains The Cloud Club Captains emphasized:\nHow to start learning AWS from zero without feeling overwhelmed. Certification preparation strategies: Cloud Practitioner Solutions Architect – Associate Importance of joining communities to accelerate learning. Common beginner mistakes: Not practicing hands-on labs Ignoring foundational principles (like the Well-Architected Framework) Lack of personal project building Tips for building a strong cloud career: Create personal projects Share notes/blogs Build a professional cloud portfolio 4. Q\u0026amp;A and Networking Activities The event included:\nOpen Q\u0026amp;A with the speakers. Discussion about effective study habits and resource selection. Networking among students from different universities. Sharing real experiences in preparing for certifications and participating in cloud projects. What I Learned 1. Cloud \u0026amp; AI Learning Mindset Cloud fundamentals are essential before diving into AI or GenAI. A clear, structured learning plan prevents burnout and wasted effort. Hands-on practice is more valuable than passive study. 2. Importance of Community in Learning AWS Cloud Clubs accelerate learning through: Study groups Peer mentoring Shared resources Networking exposes students to real opportunities in cloud engineering. 3. Certification Preparation Skills How to approach AWS exams efficiently. Useful learning tools: AWS Skill Builder Cloud Quest AWS free digital training Balancing study time and avoiding burnout. 4. AI Readiness Mindset AI is not just about models — it starts with: Data readiness Infrastructure readiness Understanding real business use cases Cloud plays a foundational role in deploying AI applications. Application to My Work Build a study plan to earn the AWS Cloud Practitioner certification. Join Cloud Club activities to expand my community and learn collaboratively. Start hands-on mini-projects such as: Deploying static website on S3/CloudFront. Building serverless APIs using AWS Lambda. Explore introductory AI services on AWS in upcoming workshops. Document my learning journey and share insights to reinforce understanding. Event Experience Attending the “AWS Cloud Club – First Cloud AI Journey” workshop was a valuable and motivating experience.\nLearning From Experienced Members Insightful stories on cloud learning paths. Practical advice on developing a discipline-based study habit. Event Atmosphere A professional yet friendly environment. Clear slides and explanations that were easy to follow. Networking Connected with peers interested in Cloud and AI. Gained exposure to multiple perspectives on cloud career development. Motivation \u0026amp; Direction The session helped me form a clearer vision of my cloud learning path. Renewed motivation to pursue cloud and AI seriously. Event Photos Overall, the event not only strengthened my technical understanding but also shaped my perspective on cloud learning, community engagement, and AI readiness. It has given me a clearer roadmap and strong motivation to move forward in my Cloud \u0026amp; AI journey.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.12-week12/",
	"title": "Worklog Week 12",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Present the team’s project to mentors and supervisors. Collect feedback for improvement on architecture, implementation, and presentation quality. Identify areas that need adjustment before the final submission. Document all comments and action items for the next sprint. Tasks to be carried out this week: Day Tasks Start Date Completion Date Resources Used 2 - Prepare final slide deck - Review speaking roles for each member 24/11/2025 24/11/2025 Slide deck 3 - Conduct internal rehearsal session - Adjust timing and transitions between presenters 25/11/2025 25/11/2025 Internal notes 4 - Official project presentation in front of mentors - Present system architecture, CI/CD pipeline, cost estimation, and demo 26/11/2025 26/11/2025 Presentation materials 5 - Receive mentor feedback on technical design, security considerations, and deployment approach - Record all comments for follow-up 27/11/2025 27/11/2025 Mentor feedback 6 - Analyze received feedback - Identify improvements required for architecture, diagrams, and slide clarity 28/11/2025 28/11/2025 Consolidated feedback 7 - Update documents and adjust the slide deck according to mentor comments - Prepare follow-up action plan for next week 29–30/11/2025 30/11/2025 Proposal, Slide deck Week 12 Achievements: Successfully presented the project to mentors and received detailed feedback on architecture, implementation, and presentation quality. Documented all comments related to: System architecture improvements Clarification of data flow and networking layers CI/CD pipeline explanation Cost optimization suggestions Security considerations Improved the slide deck and project documentation based on mentor recommendations. Identified key action items to refine before the final submission. Completed rehearsal and delivery of the mid-stage presentation as planned. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.11-week11/",
	"title": "Worklog Week 11",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Consolidate content from the team’s two proposal documents. Prepare materials for the final presentation slide deck. Standardize all information and identify the main sections to be presented. Ensure consistency between system architecture and the implementation plan. Tasks to be carried out this week: Day Tasks Start Date Completion Date Resources Used 2 - Review the entire Proposal.docx - Identify key sections to include in the presentation slides 17/11/2025 17/11/2025 Proposal.docx 3 - Consolidate Executive Summary, Problem Statement, and Solution Overview - Rewrite content in a concise, presentation-friendly format 18/11/2025 18/11/2025 Proposal.docx 4 - Extract and standardize the Solution Architecture section: + Networking layer + Application layer + CI/CD + Security \u0026amp; Monitoring 19/11/2025 19/11/2025 Proposal Template.docx 5 - Consolidate AWS Cost Estimate - Review Risk Assessment and Timeline - Convert information into short bullet points for slides 20/11/2025 20/11/2025 Proposal.docx 6 - Build the outline for the presentation slides: + Summary + Problem + Architecture + Cost + Risk + Roadmap 21/11/2025 21/11/2025 Consolidated content 7 - Standardize all content in English - Validate logical flow between sections - Prepare materials for the slide design team 22/11/2025 23/11/2025 Proposal.docx, Proposal Template Week 11 Achievements: Completed consolidation of content from both proposal documents (Executive Summary, Problem, Solution, Architecture, Risk, Cost, etc.). Built a structured outline for the presentation slide deck, clearly separating core and supplementary sections. Standardized all content in English for the final presentation. Ensured consistency between the system architecture, implementation plan, and presentation materials. Ready to move on to the slide design phase and prepare for the final presentation. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/1-worklog/1.10-week10/",
	"title": "Worklog Week 10",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Build the workshop based on the content from the Proposal and Proposal Template. Identify key sections to be included in the workshop: Overview, Architecture, CI/CD, Cost, Security, etc. Standardize the workshop structure following AWS formatting. Prepare materials so the team can present effectively during the workshop. Tasks to be carried out this week: Day Tasks Start Date Completion Date Resources Used 2 - Review the BDSS team proposal - Study the Proposal Template - Define the scope of the workshop to be built 10/11/2025 10/11/2025 proposal.docx Proposal Template.docx 3 - Analyze the workshop structure based on the template - Select relevant content from both proposals to include in the workshop 11/11/2025 11/11/2025 Proposal Template.docx 4 - Draft content for the workshop sections: Introduction, Background, Objectives - Standardize flow and wording according to AWS style 12/11/2025 12/11/2025 proposal.docx 5 - Build the main workshop content: + System Architecture + Data Flow + CI/CD Pipeline + Security Model 13/11/2025 14/11/2025 proposal.docx 6 - Consolidate AWS Cost Estimate and Risk Assessment for workshop usage - Adjust content according to the workshop layout 14/11/2025 15/11/2025 Proposal Template.docx 7 - Finalize the Workshop Draft - Clean up formatting, headings, and presentation flow - Send the completed workshop document for team review 16/11/2025 16/11/2025 — Week 10 Achievements: Completed the Workshop Draft based on both the Proposal and Proposal Template. Standardized workshop content including: Introduction, Background, Architecture, CI/CD, Cost, Security, Roadmap. Presented BDSS architecture in a clear, workshop-friendly structure. Fully consolidated cost estimates, risks, and implementation planning for workshop usage. Workshop materials are ready for team review and preparation for the upcoming presentation. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Blood Donation Support System AWS Solution for Blood Donation Support Software 1. Executive Summary Blood Donation Support System (BDSS)** is a web platform that supports the management and connection of blood donors with medical facilities. The project was developed by a group of students in Ho Chi Minh City to optimize the blood donation process, reduce the burden of searching for donors and improve the efficiency of medical communication.\nThe system is built on AWS Cloud architecture, using Amazon EC2, Amazon RDS, API Gateway, Cognito and CI/CD Pipeline (GitLab + CodePipeline) for automatic deployment. BDSS supports four user groups (Guest, Member, Staff, Admin), providing features for searching, registering for blood donation, managing blood banks, tracking blood donation processes and visual reporting.\n2. Problem Statement What’s the Problem? Healthcare facilities currently manage blood donation processes manually or through disparate tools. Finding donors who match blood type or region is difficult, especially in emergency situations. In addition, data storage systems are not synchronized, making it difficult to analyze, report and optimize blood donation campaigns.\nThe Solution Developed a comprehensive blood donation support platform on AWS Cloud, with functions for blood donation management, searching for donors and blood needers by blood type or geographic location, integrating user authentication via Amazon Cognito, and data governance on Amazon RDS. The frontend was deployed via Route 53 + CloudFront, the backend via API Gateway – EC2, a MySQL database on Amazon RDS, and an automated CI/CD pipeline using GitLab – CodePipeline.\nBenefits and Return on Investment Reduce the time it takes to find a matching donor by 60–70%. Increase the accuracy of blood type and location information. Optimize operating costs with a flexible, pay-as-you-go cloud architecture. Improve response to blood emergencies\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nThe system is divided into 4 main layers:\nEdge Networking Layer: Route 53 manages domain and DNS routing. CloudFront increases page loading speed and delivers static content. AWS WAF protects against web attacks (SQL injection, DDoS).\nApplication \u0026amp; Data Layer: Amazon EC2: Deploys backend API and handles main business. Amazon RDS (MySQL): Stores blood donor data, blood types, donation history. API Gateway: Communicates between frontend and backend. Elastic Load Balancer (ELB): Distributes load to EC2 instances. NAT Gateway \u0026amp; Internet Gateway: Supports secure Internet connection.\nCI/CD \u0026amp; DevOps Layer: GitLab: Source code management. AWS CodePipeline, CodeBuild: Deploy and update automatically.\nMonitoring \u0026amp; Security Layer: Amazon Cognito: Authentication and authorization (Guest, Member, Staff, Admin). CloudWatch, CloudTrail, IAM, Secrets Manager: Monitoring, security, system alerts. SNS: Send notifications when there is an event (blood emergency, suitable donor).\n4. Technical Implementation Implementation Phases\nAnalysis \u0026amp; Design (January) Gather requirements, define use cases, design ERD and AWS architecture. Infrastructure \u0026amp; Pipeline Setup (February) Configure Route 53, CloudFront, EC2, RDS and CI/CD on AWS. Development \u0026amp; Testing (March-April) Build main modules: blood donation registration, search, blood bank management. Integrate Cognito and SNS alert system. Deployment \u0026amp; Operation (May) Deploy the official product and monitor with CloudWatch. Yêu cầu kỹ thuật chính: Frontend: React/Next.js hoặc Angular (deploy qua S3/CloudFront). Backend: Node.js/Express trên EC2, giao tiếp qua REST API Gateway. Database: Amazon RDS MySQL, tối ưu query và backup định kỳ. CI/CD: GitLab → CodeBuild → CodePipeline → EC2. Auth: Cognito (4 vai trò: Guest, Member, Staff, Admin). Alert \u0026amp; Logs: SNS + CloudWatch + CloudTrail.\n5. Timeline \u0026amp; Milestones Timeline Phase Key Results Month 1 Requirements analysis \u0026amp; design AWS architecture + use case diagram Month 2 Infrastructure \u0026amp; pipeline setup EC2, RDS, API Gateway operational Month 3–4 Development \u0026amp; testing Key modules finalized Month 5 Live deployment System stable, with Dashboard reporting 6. Budget Estimation Services Estimated Cost/Month (USD) Notes EC2 (t2.micro) 3.50 Backend REST API Amazon RDS (MySQL) 2.80 20 GB storage API Gateway 0.50 5,000 requests CloudFront + S3 0.80 Website + CDN Route 53 0.50 Domain \u0026amp; DNS Cognito 0.10 \u0026lt;100 users CloudWatch + Logs 0.30 Monitoring \u0026amp; Alerting CI/CD (CodePipeline, CodeBuild) 0.40 Automated Deployment Total 8.9 USD/month ~106.8 USD/year Total costs may vary based on AWS Free Tier or spot instance usage.\n7. Risk Assessment Risk Impact Probability Mitigation Internet Outage Medium Medium Redundancy on EC2 Backup DDoS Attack High Low AWS WAF + CloudFront User Data Corruption High Low RDS Backup + IAM Restricted Access Cost Overrun Medium Low AWS Budget Alert CI/CD Deployment Disruption Low Medium Pipeline Testing Before Merging 8. Expected Outcomes Technology: Cloud-native system, automatic CI/CD, multi-user support and high security. Application: Helps medical facilities manage blood donations effectively, minimizing manual processes. Expansion: Can be replicated for many other hospitals, integrating AI to analyze blood group needs or predict upcoming blood donations.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint. In Create endpoint console: Name the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. First cloud journey Lab for indepth understanding of Session manager. In the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Simplifying Multi-Tenant Encryption with a Cost-Efficient AWS KMS Key Strategy Introduction Organizations face diverse challenges when it comes to managing encryption keys. While some scenarios require strict separation, there are practical use cases where a centralized approach can streamline operations and reduce complexity. In this post, we focus on Software-as-a-Service (SaaS) providers, but the principles can apply to large enterprises facing similar key-management challenges.\nManaging encryption in a multi-tenant, multi-service architecture presents significant complexity. Many organizations struggle with the overhead and cost of provisioning separate AWS Key Management Service (AWS KMS) customer managed keys for each tenant and each service. While secure, this approach often increases operational overhead and KMS usage costs over time.\nBut what if there were a more efficient way?\nIn this article, we introduce a strategy for using a single customer-managed (symmetric) key per tenant across your services. After reading this post, you will learn:\nHow to implement a scalable, secure, and cost-efficient encryption model Techniques for sharing a single customer-managed key per tenant across multiple services and environments How to encrypt tenant data stored in Amazon DynamoDB and other storage types while maintaining strong tenant isolation Multi-Tenant Encryption Requirements in SaaS Data isolation is a foundational requirement in SaaS multi-tenant architectures, supporting compliance and customer trust. Many SaaS providers must encrypt sensitive information—such as API keys, credentials, and personal data—in storage solutions like DynamoDB and Amazon Simple Storage Service (Amazon S3).\nAlthough these services provide default encryption at rest, they often use a single shared key across many data records. Consider DynamoDB in a shared-pool model, where a single table stores data for multiple tenants. In this setup, tenant data is encrypted using the same AWS KMS key regardless of ownership.\nA KMS key represents a top-level key container uniquely identified in AWS KMS. For more details about the hierarchy of keys involved when encrypting or decrypting data, see the AWS KMS key hierarchy.\nThis shared-key approach is often insufficient for SaaS providers operating under strict security or compliance frameworks. Some customers require:\nBring Your Own Key (BYOK) Logical data isolation via dedicated encryption keys To meet these demands, a provider may create dedicated AWS KMS customer managed keys for each customer, ensuring their sensitive data remains isolated and inaccessible to other tenants.\nProviders may also consider a silo model—separate tables per customer. However, that approach introduces challenges: as the customer base grows, managing numerous tables becomes increasingly complex, and AWS service quotas can become limiting.\nScaling Concerns: Managing KMS Keys at Large Scale As a SaaS platform grows, empowering service teams to develop independently is crucial. A common scaling pattern is enabling each team to build services in dedicated AWS accounts. This often leads to a decentralized model where each service manages its own KMS keys per customer.\nHowever, this autonomy introduces hidden costs as both your customer base and service catalog expand.\nThe Challenge of Key Proliferation As the organization grows, the number of keys increases with each new tenant and service. This results in several challenges:\nCost impact: Each AWS KMS key costs $1 per month, up to $3 per month with two or more key rotations. Operational complexity: Managing many KMS keys across environments and accounts becomes error-prone and hard to scale. Organizational inefficiency: Duplicate engineering efforts as teams build and maintain similar key-management logic. Governance cost: Enforcing consistent policies and tracking KMS usage across multiple AWS accounts becomes difficult. A Streamlined Approach The solution lies in adopting a centralized key-management strategy—one KMS key per tenant, stored in a central AWS account. This approach effectively addresses cost, operational, and governance challenges while maintaining strong security guarantees.\nIn the following sections, we\u0026rsquo;ll explore how to implement this centralized model and securely share KMS keys across services and AWS accounts.\nSolution Overview: Centralized Tenant Key Management At the core of the solution is a centralized Tenant Key Management Service (shown as Service A in the illustration). This service manages the entire life cycle of tenant KMS keys—from creation during tenant onboarding to alias management, access policies, and deletion.\nThe service enables safe, scalable cross-organization use of keys through cross-account AWS Identity and Access Management (IAM) access. It grants other services (such as a customer-facing service in Account B) permissions to perform specific encryption operations using the tenant\u0026rsquo;s KMS key via role delegation. The design follows AWS best practices for cross-account IAM access using AWS Security Token Service (AWS STS), as described in AWS documentation and related blog posts.\nCentralized Key Management in Practice: Encrypting Customer Data Let’s examine how this works in practice with a common scenario:\nService A: The centralized tenant key-management service in Account A Service B: A customer-facing workload running in Account B When a customer interacts with Service B, sensitive information—such as secrets, API keys, or license information—must be stored securely in a DynamoDB table. Instead of relying on a shared KMS key or default encryption, Service B encrypts data using the customer\u0026rsquo;s dedicated KMS key managed by Service A.\nThis process uses cross-account IAM role delegation. Service B temporarily assumes a role (ServiceARole) in Account A, gaining scoped permissions for the specific tenant’s KMS key. Using these temporary credentials, Service B can perform client-side encryption using AWS SDK or AWS Encryption SDK.\nSolution Walkthrough Assumptions and definitions:\nIncoming requests include an authentication header with a JWT containing the tenant identifier (). Account A: Centralized key-management service Account B: Customer-facing service alias/customer-: The alias format in Account A, mapped to each tenant\u0026rsquo;s KMS key ServiceARole: A role in Account A allowed to encrypt and decrypt using tenant-key aliases ServiceBRole: A role in Account B that may assume ServiceARole Let’s walk through the flow:\nUsing the Service with JWT A tenant-associated customer logs into the SaaS application and receives a JWT containing their tenant ID. The customer performs an action in Service B and sends sensitive data.\nService B processes the request, verifies the JWT, and must:\nEncrypt the customer\u0026rsquo;s sensitive data Store the encrypted value along with other data in DynamoDB Assuming the Role The Lambda execution role in Service B assumes ServiceARole in Account A. An alternative cross-account access method is KMS grants.\nExample IAM policy for ServiceARole, allowing encryption/decryption based only on alias/customer-*:\n{ \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;AllowKMSByAlias\u0026rdquo;, \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: [ \u0026ldquo;kms:Encrypt\u0026rdquo;, \u0026ldquo;kms:Decrypt\u0026rdquo;, \u0026ldquo;kms:GenerateDataKey*\u0026rdquo; ], \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;Condition\u0026rdquo;: { \u0026ldquo;StringLike\u0026rdquo;: { \u0026ldquo;kms:RequestAlias\u0026rdquo;: \u0026ldquo;alias/customer-\u0026rdquo; } } } ] }\nTo securely encrypt tenant secrets at scale, we grant application roles cross-account access to the KMS key—but only through their alias, which maps to the tenant identifier contained in their JWT authentication token, enforcing strong isolation.\nYou can control access to the KMS key based on the aliases associated with each KMS key. To do this, use the kms:RequestAlias ​​and kms:ResourceAliases conditional keys as specified in Use aliases to control access to KMS keys.\nAdditionally, the ServiceARole trust policy allows ServiceBRole in account B to assume: { \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;: { \u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;arn:aws:iam::\u0026lt;ACCOUNT_B_ID\u0026gt;:role/ServiceBRole\u0026rdquo; }, \u0026ldquo;Action\u0026rdquo;: \u0026ldquo;sts:AssumeRole\u0026rdquo; } ] }\nDepending on your environment, you can add additional conditions to this trust policy to further narrow the scope of who can assume this role. For more information, see IAM and AWS STS condition context keys.\nEach customer-managed KMS key will then have the following policy. For example, a KMS key for a customer with : 123 will have a policy that restricts access to the key using a specific customer alias and only through ServiceRoleA. { \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Id\u0026rdquo;: \u0026ldquo;TenantKeyPolicy\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;AllowServiceARoleViaAlias\u0026rdquo;, \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;: { \u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;arn:aws:iam::\u0026lt;ACCOUNT_A_ID\u0026gt;:role/ServiceARole\u0026rdquo; }, \u0026ldquo;Action\u0026rdquo;: [ \u0026ldquo;kms:Encrypt\u0026rdquo;, \u0026ldquo;kms:Decrypt\u0026rdquo;, \u0026ldquo;kms:GenerateDataKey*\u0026rdquo; ], \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;*\u0026rdquo;, \u0026ldquo;Condition\u0026rdquo;: { \u0026ldquo;StringLike\u0026rdquo;: { \u0026ldquo;kms:RequestAlias\u0026rdquo;: \u0026ldquo;alias/customer-123\u0026rdquo; } } } ] }\nHere is a Python code example that illustrates how Service B automatically assumes a role in Account A to encrypt data for a specific tenant using a session-scoped IAM policy that only allows access to that tenant\u0026rsquo;s KMS key alias.\nThis pattern follows the same principles outlined in Isolating SaaS Tenants with Dynamically Generated IAM Policies. The idea is to create and attach a tenant-specific IAM policy at runtime that grants the minimum permissions required to operate on tenant-owned resources—in this case, a KMS key alias. The credentials will allow the Lambda function to only use the KMS key that belongs to the customer (identified by tenant_id).\nWe\u0026rsquo;ll call assume_role_for_tenant for every tenant.\nStatus of \u0026ldquo;StringEquals\u0026rdquo; - \u0026ldquo;kms:RequestAlias\u0026rdquo;: alias is the magic formula of AWS STS, it restricts ServiceB from using the current tenant\u0026rsquo;s alias in its encrypted SDK calls and relies on alias authorization\nimport boto3 def assume_role_for_tenant(tenant_id: str): alias = f\u0026quot;alias/customer-{tenant_id}\u0026quot; # Session policy scoped to only the specific alias session_policy = { \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: [ \u0026ldquo;kms:Encrypt\u0026rdquo;, \u0026ldquo;kms:Decrypt\u0026rdquo;, \u0026ldquo;kms:GenerateDataKey*\u0026rdquo; ], \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;*\u0026rdquo;, \u0026ldquo;Condition\u0026rdquo;: { \u0026ldquo;StringEquals\u0026rdquo;: { \u0026ldquo;kms:RequestAlias\u0026rdquo;: alias } } } ] } # Assume ServiceARole in Account A with inline session policy sts = boto3.client(\u0026ldquo;sts\u0026rdquo;) assumed = sts.assume_role( RoleArn=\u0026ldquo;arn:aws:iam::\u0026lt;ACCOUNT_A_ID\u0026gt;:role/ServiceARole\u0026rdquo;, RoleSessionName=f\u0026quot;Tenant{tenant_id}Session\u0026quot;, Policy=json.dumps(session_policy) ) return assumed[\u0026ldquo;Credentials\u0026rdquo;]\nEncrypt data and store it in DynamoDB Now, all that\u0026rsquo;s left to do is use the assumed role credentials and use the AWS SDK to encrypt sensitive customer data and store it in a DynamoDB table.\nUse temporary credentials to create a KMS client creds = assume_role_for_tenant(tenant_id, plaintext)\rkms = boto3.client(\r\u0026quot;kms\u0026quot;,\rregion_name=\u0026quot;us-east-1\u0026quot;,\raws_access_key_id=creds[\u0026quot;AccessKeyId\u0026quot;],\raws_secret_access_key=creds[\u0026quot;SecretAccessKey\u0026quot;],\raws_session_token=creds[\u0026quot;SessionToken\u0026quot;]\r)\r# Encrypt using the alias\rresponse = kms.encrypt(\rKeyId= f\u0026quot;alias/customer-{tenant_id}\u0026quot;\rPlaintext=plaintext\r)\r# store response[\u0026quot;CiphertextBlob\u0026quot;] in DynamoDB table\rThis article is not about isolation between different services, only isolation between tenants. If you need such service isolation, you can use an encryption context, an optional set of non-secret key/value pairs that can contain additional contextual information about the data, such as a service identifier. This helps ensure that services can only encrypt or decrypt data using their respective service encryption context.\nBenefits of Centralized Key Management Let\u0026rsquo;s see how this solution addresses our previous challenges.\nTenant Isolation Design While reducing the total number of KMS keys, we still maintain strict tenant isolation. Each customer\u0026rsquo;s sensitive data is still encrypted with a dedicated key, identified by a unique alias (alias/customer-). Access control to the tenant key is tightly managed through IAM role delegation, following the principles of least privilege:\nService A has exclusive control over the tenant\u0026rsquo;s KMS key management.\nService B can only assume the role of granting restricted encryption, decryption, and GenerateDataKey access to the customer-managed key specified by the alias: alias/customer-.\nOptimized Cost Management Our approach significantly reduces costs by moving from multiple service-specific KMS keys per tenant to a single KMS key per tenant, securely shared across multiple services and environments. This approach introduces a new centralized account (Account A) that provides access to encryption keys in appropriate circumstances. It is important to understand AWS STS limits, specific to calls, and consider mechanisms for storing temporary IAM credentials if those limits become a bottleneck. Additionally, if KMS limits are a bottleneck, consider using data key caching using the AWS Encryption SDK.\nStreamlined Operations and Governance By centralizing key management in Service A, you can achieve:\nConsistent KMS key lifecycle management across the organization Improved auditability by using AWS CloudTrail to better understand key access patterns by service Reduced operational costs Simplified compliance monitoring The only additional complexity is setting up initial cross-account role delegation between Service A and other services. Once established, this framework can be extended to accommodate new subscribers and services.\nIt is best to encapsulate the logic for role assignment, policy generation, and AWS SDK client initialization in a common SDK for the entire organization. This abstraction reduces the cognitive load on developers and minimizes the risk of misconfiguration. You can go further by providing high-level utility functions like encrypt_tenant_data() and decrypt_tenant_data(), which hide the underlying complexity while promoting secure and consistent usage patterns across the team.\nConclusion In this article, we explored an effective approach to managing encryption keys in multi-tenant SaaS environments through centralization. We looked at common challenges faced by growing SaaS providers, including key proliferation, rising costs, and operational complexity across multiple AWS accounts and services. This centralized key management solution uses AWS best practices for IAM role delegation and cross-account access, allowing organizations to maintain security and compliance while minimizing operational costs. By implementing this approach, SaaS providers or large organizations facing similar challenges can effectively manage their encryption infrastructure as they scale, without compromising security or increasing complexity.\nAbout the Authors Itay Meller is a Security Specialist Solutions Architect at AWS, with a strong background in cybersecurity R\u0026amp;D and leadership roles at multiple security-focused companies. With extensive expertise in cloud security, Itay helps organizations securely adopt and scale AWS environments by solving complex security and compliance challenges.\nRan Isenberg is an AWS Serverless Hero, Principal Software Architect at CyberArk, blogger and speaker. He maintains the blog RanTheBuilder.cloud, where he shares his knowledge and experience in the Serverless world.\nYossi Lagstein is a Senior Solutions Architect at Amazon Web Services. Yossi has over 30 years of experience as an expert and manager developing infrastructure components for a variety of projects and products. Yossi helps AWS customers develop, design, and build well-architected solutions. Outside of work, Yossi enjoys running, swimming, and hiking.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you participated in during your internship or work experience.\nEach event should follow the format Event 1, Event 2, Event 3… and include:\nEvent name Time of the event Location (if any) Your role in the event A brief description of the content and key activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This helps demonstrate your actual involvement as well as the soft skills and experience you gained from each event.\nDuring my internship, I participated in two events. Each event brought valuable knowledge, practical insights, and memorable experiences.\nEvent 1 Event Name: AWS Mastery #2 – CloudFormation \u0026amp; CDK Workshop\nTime: November 17, 2025\nLocation: Online event (AWS Community Vietnam)\nRole: Participant\nEvent Description:\nThe workshop focused on Infrastructure as Code (IaC) using CloudFormation and AWS CDK. Speakers introduced IaC concepts, explained the structure of CloudFormation templates, demonstrated commands such as cdk deploy and cdk diff, and guided participants on deploying infrastructure through code. Additional topics included Docker fundamentals, containerization, and services like ECS, EKS, and App Runner.\nValue Gained:\n- A solid understanding of IaC workflows, drift detection, and CloudFormation best practices.\n- Knowledge of CDK constructs (L1–L3) and automated infrastructure deployment.\n- Practical insights into container orchestration on AWS.\n- Improved ability to analyze and deploy DevOps pipelines using IaC.\nEvent 2 Event Name: AWS Cloud Club – First Cloud AI Journey Workshop\nTime: November 29, 2025\nLocation: Offline event for Cloud Clubs (Ho Chi Minh City)\nRole: Participant\nEvent Description:\nThe workshop introduced the AWS Cloud Club community and provided a learning roadmap for Cloud + AI for students. Cloud Club Captains shared their AWS learning journeys, covering cloud fundamentals, AI/ML basics, GenAI concepts, study resources, certification paths, and practical career-development guidance.\nValue Gained:\n- A clearer understanding of why cloud fundamentals are essential before diving into AI/GenAI.\n- A structured AWS learning roadmap (Cloud Practitioner → Solutions Architect Associate).\n- Effective self-study strategies and common mistakes beginners should avoid.\n- Expanded professional network through connections with Cloud Club members and peers interested in cloud and AI.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Accelerate your Cloud Strategy with Megaport\u0026rsquo;s 25 Gbps Hosted AWS Direct Connect This blog introduces how to accelerate your cloud strategy with Megaport\u0026rsquo;s 25 Gbps Hosted AWS Direct Connect. You will learn the benefits of performance, scalability, and secure network connectivity between on-premises and the cloud, and how to deploy and manage this connectivity to optimize costs and operational efficiency.\nBlog 2 - Simplify multi-subscriber encryption with cost-effective AWS KMS key strategies This blog guides you through how to simplify multi-subscriber encryption with cost-effective AWS KMS key strategies. You will learn how to manage and distribute KMS keys efficiently, optimize costs, ensure data security on a per-tenant basis, and maintain performance and scalability in a multi-tenant environment.\nBlog 3 - Open Protocol for Agent Interoperability Part 4: A2A Agent Communication This blog introduces the Agent-to-Agent (A2A) protocol that enables AI agents to communicate directly with each other. You will learn how AWS supports A2A through the Strands Agents SDK, key features such as Agent Tags, structured task execution, multiple transport options, and A2A security. The article also illustrates example deployments of HR agents and Employee Information agents using A2A, and customer feedback on multi-agent interoperability.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints. Click the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use. Connect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box Click Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $ Change to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo; Create a file named testfile2.xyz fallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://phuong721.github.io/learning-aws/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Open Protocol for Agent Interoperability Part 4: A2A Agent Communication Introduction: Welcome to Part 4 of our Open Protocols for Agent Interoperability blog series where we will cover the Agent-to-Agent (A2A) protocol, AWS\u0026rsquo;s involvement with the Linux Foundation-based open standard, and our support for A2A in the Strands Agents SDK. Here\u0026rsquo;s what we\u0026rsquo;ve covered so far:\nPart 1: How the Model Context Protocol (MCP) facilitates communication between agents and how AWS has worked to improve the MCP specification to better support agent-to-agent communication.\nPart 2: Details on recent MCP specification updates related to Authentication.\nPart 3: How to build inter-agent systems with the new Strands Agents SDK and MCP\nStandard protocols are the primary way to connect network services. Typically, there are different protocols to address different types of network connectivity. At the network layer, there are two main protocols: TCP and UDP. Each protocol is suited to specific needs, and neither protocol is universal. This is also true when connecting AI agents. In part 4 of our agent-to-agent communication series, we will introduce A2A, how it can be used for agent-to-agent communication, and how AWS helps customers build systems with A2A.\nMCP was originally created to connect agents to tools, but can also be used to connect agents to each other. A2A was created to connect agents to each other, and can also be used in conjunction with MCP for agents to communicate with tools. Which protocol to use for agent-to-agent communication depends on your needs. AWS supports both protocols, allowing customers to deploy their code on AWS using MCP, A2A, or a combination of both.\nAs inter-agent protocols and the frameworks around them evolve, they will likely become more like TCP \u0026amp; UDP, where most developers focus more on building their agents rather than the underlying protocols. The first step towards that is for agent frameworks to support the protocols and build ecosystems around them. At AWS, we have taken this first step by joining the A2A standards community and adding support for A2A in our open-source Strands Agents SDK. Swami Sivasubramanian, Vice President of AWS Agentic AI, summarizes this effort:\nAt AWS, we believe agentic AI will be critical to nearly every customer experience. We welcome A2A to the Linux Foundation and expect this to create more opportunities for anyone building AI applications. We plan to support the community with project contributions and access to the broadest and deepest set of agentic frameworks, protocols, and services.\nSimilar to how we are supporting the development of MCP, we are supporting the development of A2A to meet customer needs. We plan to focus on several areas to make A2A work well on AWS, including support for Amazon Bedrock AgentCore, extending the A2A protocol for temporary task storage and SigV4, improving multi-task management, and improving the Java A2A SDK.\nOverview of A2A The A2A protocol addresses a key challenge in the AI ​​landscape. It enables AI agents built on diverse platforms, operated by different companies on separate servers, to communicate and collaborate effectively—as agents, not just tools. A2A represents a significant step forward in creating interoperable AI systems that work together across organizational boundaries. The protocol is supported by a growing partner ecosystem, including more than 50 technology companies such as Google, Atlassian, Confluent, Salesforce, SAP, and MongoDB.\nBefore A2A, organizations faced significant challenges in deploying multi-agent AI systems at scale. Without a standardized protocol, each pair of agents required custom integration code, leading to excessive development costs and maintenance complexity. This creates siloed AI systems where specialized agents cannot easily share capabilities or coordinate on complex tasks. The protocol provides agents with a common language, allowing them to maintain their autonomy and specialized skills while collaborating—agents communicate with each other as peers, not just tools. This distinction is important because it allows agents to engage in complex back-and-forth interactions, negotiate task requirements, and maintain independent decision-making while working toward common goals.\nFor AWS customers, A2A offers a number of compelling features that align with enterprise requirements. The protocol enables enterprise capabilities including secure agent discovery through standardized agent tags, authentication and authorization mechanisms for controlled access, support for multiple communication methods (text, forms, media), and the ability to allow agents to collaborate on long-running tasks without revealing their internal state or implementation details.\nA2A addresses the unique challenges of multi-agent collaboration through features that enable complex workflows, handling complex real-world business processes with the observability and control required by production environments. Specifically, it supports agent tags, structured tasks, multiple transport options, and authentication/authorization primitives.\nAgent Tags Effective multi-agent communication requires agents to discover and understand each other’s capabilities. A2A supports this through Agent Tags — metadata documents that capture semantic meaning about what each agent can do, how it prefers to operate, and what types of tasks it is good at. Agent Tags enable other agents to make intelligent decisions about when and how to cooperate. Agent Tags also describe an agent’s authentication/authorization requirements and support incrementally expanded capabilities after authentication, using Authenticated Extended Agent Tags.\nStructured Task Execution Agents work together to solve problems by leveraging tasks to structure their communication; they organize their messages into intelligent units of work, carry context, track progress, and store output artifacts. Through tasks, agents can reference previously created artifacts, understand dependencies between tasks in a workflow, and make informed decisions based on the entire conversation history. Tasks support both sequential and parallel execution for complex workflows. Agents can create multiple subsequent tasks simultaneously and create dependent activity chains. This gives application developers the flexibility to model real-world business processes.\nBy leveraging task IDs and context, applications can trace task origins, tracing task chains back to their roots to recover information about how outputs are produced. This improves observability by providing agents with the ability to generate rich activity logs for debugging and auditing.\nMultiple Transport Options A2A empowers application developers by supporting three core protocols with equivalent capabilities: JSON-RPC 2.0, gRPC, and REST. This allows developers to choose the transport that best fits their team’s expertise, existing infrastructure, and performance requirements. For long-running operations, A2A enhances each transport with Server-Sent Events (SSE) for streaming and sending webhook-based push notifications. Developers have intuitive options for handling asynchronous task updates and real-time progress monitoring without complex polling logic.\nA2A Security Enterprise-grade security is a non-negotiable requirement for agent systems. A2A enables a robust security architecture by supporting a number of authentication protocols; including OAuth 2.0, OpenID Connect, and mTLS, allowing organizations to integrate agents with their existing identity infrastructure, while skill-specific authorization metadata and secondary authentication support allow for fine-grained access control policies that can be enforced at the application level.\nA2A’s decision to keep agents opaque to each other supports a zero-trust architecture by treating each agent as an independent security boundary, and the protocol’s support for task auditing provides the foundation for comprehensive security monitoring and compliance reporting.\nInter-Agent with Strands Agents \u0026amp; A2A The unique features of A2A make it perfect for interoperability between agent platforms. Several open source agent platforms already support it. The open source Strands Agents SDK recently added support for A2A so agents can easily communicate with other agents.\nStrands Agents takes a model-driven approach to building and operating AI agents in just a few lines of code. Strands scales from simple to complex agent use cases, and from local development to production deployment. Many teams at AWS have used Strands for AI agents in production, including Amazon Q Developer, AWS Glue, and VPC Reachability Analyzer.\nWith built-in A2A support in Strands Agents, you can easily use an agent as an A2A server and communicate from one Strands Agent to other A2A agents. To illustrate this, consider the example of a Human Resources (HR) agent that can answer questions about employees. To do this, you can imagine the HR agent communicating with several other agents such as an employee data agent, an Enterprise Resource Planning (ERP) agent, a performance agent, a goals agent, etc. For this example, let\u0026rsquo;s start with a basic architecture, where a REST API provides access to an HR agent that connects to an Employee Info agent: The architecture of the inter-agent system consists of two agents (HR \u0026amp; Employee Info), connected using A2A.\nNote: The complete, working version of the following example is available in our Agentic AI samples repo.\nOur employee information tool uses Amazon Bedrock and the MCP tool to retrieve employee data (see full code for those aspects): employee_agent = Agent( model=bedrock_model, name=\u0026ldquo;Employee Agent\u0026rdquo;, description=\u0026ldquo;Answers questions about employees\u0026rdquo;, tools=tools, system_prompt=\u0026ldquo;you must abbreviate employee first names and list all their skills\u0026rdquo; )\nTo expose this agent via A2A, we just need to create an A2A server and start it when the program runs: a2a_server = A2AServer(agent=employee_agent, host=urlparse(EMPLOYEE_AGENT_URL).hostname, port=urlparse(EMPLOYEE_AGENT_URL).port)\nif name == \u0026ldquo;main\u0026rdquo;: a2a_server.serve(host=\u0026ldquo;0.0.0.0\u0026rdquo;, port=8001)\nNote that we pass EMPLOYEE_AGENT_URL via an environment variable. This helps our infrastructure definition know the endpoint URL that can set the host and port used in the A2A agent tag (used by the A2A client to discover the agent).\nNow we can access the Employee Info agent via A2A and we can create the HR agent: provider = A2AClientToolProvider(known_agent_urls=[EMPLOYEE_AGENT_URL]) agent = Agent(model=bedrock_model, tools=provider.tools)\nThis agent can now be invoked in a variety of ways. In this example, we invoke it from a REST request. See the full code for the REST aspects. Here\u0026rsquo;s what happens when a REST request is made:\nThe user (possibly via a web or mobile app) sends a query like \u0026ldquo;list employees with AI-related skills\u0026rdquo;\nThe HR agent uses the Amazon Nova model to understand the user\u0026rsquo;s query and decides that it should be sent to the employee information agent.\nUsing A2A, the query is sent to the Employee Information agent.\nThe Employee Information agent uses the Amazon Nova model to understand the query and decides that it should call the Employee Data MCP server.\nThe Employee Information agent calls the Employee Data MCP server to query the employee database and return the data to the Nova model.\nGiven a system requirement to abbreviate employee names, the model takes a list of employees, formats the list nicely, abbreviates the names, and returns the text to the Employee Info agent.\nThe Employee Info agent returns the text to the HR agent, which returns the text in a REST response.\nOf course, all of this can run on AWS using a variety of runtime environments (Amazon Elastic Kubernetes Service (Amazon EKS), Amazon Elastic Container Service (Amazon ECS), Amazon Bedrock AgentCore, AWS Lambda, etc.). This example contains an AWS CloudFormation deployment template that deploys the agents and MCP servers on Amazon ECS (all in a VPC) and an Application Load Balancer to expose the REST service.\nWe can experiment with curl: curl -X POST \u0026ndash;location \u0026ldquo;http://something.us-east-1.elb.amazonaws.com/inquire\" -H \u0026ldquo;Content-Type: application/json\u0026rdquo; -d \u0026lsquo;{\u0026ldquo;question\u0026rdquo;: \u0026ldquo;list employees that have skills related to AI programming\u0026rdquo;}\u0026rsquo; And we are back: Here are the employees with skills related to AI programming:\nA. Rosalez - Machine Learning, REST API E. Owusu - DevOps, Machine Learning, Python J. Doe- Machine Learning, JavaScript K. Mensah - REST API, Kubernetes, Machine Learning, Node.js M. Rivera - AWS, Kubernetes, GraphQL, Machine Learning M. Major - MongoDB, Angular, Kotlin, Machine Learning, REST API C. Salazar - React, Machine Learning, SQL, Kotlin N. Wolf - SQL, Machine Learning, Docker, DevOps, Git If you need more If you need detailed information about any of these employees or require further assistance, please let me know!\nGet the complete source for this example.\nWith Strands Agents, it takes just a few lines of code to expose agents as A2A servers and communicate between agents using A2A. In future posts, we will cover more advanced agent types such as Swarms, Graphs, and Workflows.\nCustomer Perspectives We’ve heard from a number of customers and partners who are excited about our A2A support. Here are some of their thoughts:\n“At Autodesk, we are committed to driving open standards for agent AI and interoperability as we shape the future of design and engineering. Through our collaboration with AWS and the A2A community, we are excited to help build an ecosystem where intelligent agents can communicate seamlessly across Autodesk platforms. As we continue to enhance Autodesk Platform Services with generative AI capabilities, we see tremendous potential in how interoperable AI agents can transform workflows across architecture, engineering, construction, and manufacturing. Working with AWS, we are committed to creating solutions that enable secure and efficient agent collaboration while maintaining enterprise-grade standards.” – Ritesh Bansal, Vice President of Data Analytics, Insights and AI/ML Platform, Autodesk\n“Our commitment to developing Agentic AI, open protocols, and interoperability is at the core of our vision for secure and intelligent networks. By collaborating with AWS and the A2A community, we are driving innovation to set new standards for AI-based security, enabling organizations to operate with greater resilience and confidence in the rapidly evolving AI era.” – Raj Chopra, Senior Vice President and Chief Product Officer, Security, Cisco\n“As organizations design increasingly sophisticated agentic AI systems, coordination between agents and tools is becoming essential. We are excited to see AWS accelerate efforts like A2A, supporting better interoperability architectures that help organizations and Datadog customers build more observable, reliable, and secure agent-based applications.” — Yrieix Garnier, Vice President of Product, Datadog\n“MongoDB and AWS are committed to building an open, interoperable ecosystem that gives developers greater freedom to innovate. Adopting open standards like A2A is an important step toward this vision, simplifying how agents interact with MongoDB\u0026rsquo;s rich document model, built-in vector search, and Voyage AI models.” – Abhinav Mehla, Vice President of Global Partner Programs \u0026amp; Ecosystem, MongoDB\n“Interoperability is critical for AI agents to work seamlessly and efficiently across enterprise systems and tools, which is why we collaborated with the industry to develop the A2A standard, and why we will support open standards like A2A and MCP in Agentforce. AWS’s support for A2A will continue to help break down barriers between vendors, drive innovation, and deliver significant value to our mutual customers by enabling agents to work across the company’s entire infrastructure and ecosystem of tools and agents.” — Gary Lerhaupt, Vice President of Product Architecture, Salesforce\n“It’s exciting to see industry leaders like AWS back the Agent2Agent protocol. What started as a bold idea is now quickly becoming an industry standard – one based on openness, security, and cross-platform interoperability. With support from AWS and other partners, the A2A ecosystem is truly thriving, and ServiceNow is proud to lead this trend by making enterprise-grade, interoperable AI agents a reality.” – Joe Davis, Executive Vice President of Platform Engineering \u0026amp; AI Technologies at ServiceNow.\nSnowflake firmly believes that some of the industry’s biggest innovations come from open protocols and the communities that support them. Maximizing the potential of agentic AI depends on open protocols like A2A, as well as the shared knowledge and best practices they provide. We are excited to see AWS demonstrate their commitment to open protocols for agent interoperability by adding A2A support to Strands Agents. Together with the support of the broader technology community, the industry will be able to automate knowledge work with secure agentic systems like Strands Agents and Snowflake Cortex Agents. – Dwarak Rajagopal, Vice President of AI Engineering \u0026amp; Research, Snowflake\n“In the future, a fragmented workforce of humans and AI agents will inevitably hinder growth. We believe that open protocols, particularly Agent-to-Agent (A2A) Protocols, play a critical role in the evolution of this hybrid workforce. They enable secure, collaborative communication, ensuring interoperability across diverse agent ecosystems. Workday’s Agent System of Record (ASOR) uniquely extends our trusted platform to manage people, finances, and agents together. Working with AWS and the A2A community, we are committed to enabling secure, interoperable agent communication. This isn’t just about new technology; it’s about securely unlocking new levels of productivity and innovation across the enterprise, while maintaining complete control.” —Dean Arnold, Vice President of System of Record, Workday\nGetting Started and Providing Feedback To get started building interactive AI agents using A2A, check out the Strands Agents A2A docs. We\u0026rsquo;d love to hear your feedback on using A2A with Strands Agents! Join the discussions on the open source Strands Agents Python SDK repo to let us know what else you need to know when building an inter-agent system.\nNick Aldridge is a Principal Engineer at AWS. Over the past 6 years, Nick has been involved in multiple AI/ML initiatives, including Amazon Lex and Amazon Bedrock. Most recently, he led the launch of Amazon Bedrock Knowledge Bases. Currently, he works on generative AI and AI infrastructure, focusing on agent collaboration and function invocation. Before joining AWS, Nick earned his Master\u0026rsquo;s degree from the University of Chicago.\nJames Ward is a Principal Developer Advocate at AWS. James travels the world helping enterprise developers learn how to build reliable systems. His current focus is on helping developers build systems of AI agents using Spring AI, Embabel, Strands Agents, Amazon Bedrock, MCP, and A2A.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://phuong721.github.io/learning-aws/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at [First Cloud AI Journey Office] from [08/09/2025] to [30/12/2025], I had the opportunity to participate in [description of main project or tasks]. This experience helped me improve my abilities in [list of skills: data analysis, programming, planning, presenting, teamwork, etc.].\nI consistently maintained a learning mindset, stayed proactive in my work, and collaborated effectively with colleagues to achieve the team’s shared goals. Below is my detailed self-evaluation based on the given criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge and skills Understanding the work, applying knowledge to practice, using appropriate tools ✅ ☐ ☐ 2 Ability to learn Quickly absorbing new knowledge, being proactive in learning from real-life situations ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking new tasks without reminders ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time, ensuring work quality ✅ ☐ ☐ 5 Discipline Following schedules, workflows, and organizational rules ☐ ✅ ☐ 6 Motivation for improvement Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas clearly and understandably, providing effective feedback ☐ ✅ ☐ 8 Teamwork Collaborating well with colleagues, contributing to the team ✅ ☐ ☐ 9 Professional conduct Maintaining respectful behavior toward colleagues and the work environment ✅ ☐ ☐ 10 Problem-solving skills Analyzing issues and proposing feasible, creative solutions ☐ ✅ ☐ 11 Contribution to the project/organization Level of task completion and positive contributions to overall progress ☐ ✅ ☐ 12 Overall performance General assessment of effort, adaptability, and improvement during the internship ✅ ☐ ☐ Areas for Improvement Strengthen time-management skills to work more efficiently Improve analytical skills and the ability to solve complex problems Be more proactive in internal communication to enhance team collaboration "
},
{
	"uri": "https://phuong721.github.io/learning-aws/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/7-feedback/",
	"title": "Feedback &amp; Suggestions",
	"tags": [],
	"description": "",
	"content": "General Feedback 1. Working Environment\nDuring my training at AWS, I found the working environment to be highly professional and dynamic. Everyone communicates clearly and directly, making discussions comfortable and efficient. What I appreciated the most is the “learning by doing” culture – encouraging hands-on practice rather than relying solely on theory. However, I think AWS could consider organizing more internal sharing sessions so interns can better understand the responsibilities of different teams.\n2. Support from Mentor / Admin Team\nMy mentor was extremely supportive, providing clear guidance before each task. Whenever I encountered difficulties, the mentor didn’t solve the problem for me but instead helped me think in the right direction – which significantly improved my problem-solving skills. The admin team also responded quickly, assisting with accounts, permissions, and documentation whenever needed.\n3. Relevance Between Tasks and My Major\nThe training topics were closely related to my major in Information Security, especially IAM, cloud security architecture, and AWS resource management. I also had the opportunity to work with technologies and tools that are not covered in my university curriculum, which helped broaden my knowledge considerably.\n4. Learning Opportunities \u0026amp; Skill Development\nThroughout the internship, I gained various new skills such as log analysis, applying AWS best practices, handling simulated incidents, and managing cloud resources. Besides technical skills, I also improved my communication, planning, and time-management abilities thanks to working with sprints and clear deadlines.\n5. Culture \u0026amp; Team Spirit\nThe team spirit was excellent. Everyone was willing to help each other and openly share their experiences. The working environment was professional yet friendly, making it easy to interact and collaborate. This helped me integrate quickly and become more confident during technical discussions.\n6. Policies / Benefits for Interns\nThe policies for interns were clear and flexible. The working schedule was well-adjusted to accommodate my academic commitments. Moreover, being allowed to participate in internal workshops and technical training sessions was a big advantage.\nAdditional Questions What were you most satisfied with during the internship?\n→ Having the chance to work directly with real AWS services and receiving timely, constructive support from my mentor.\nWould you recommend this internship to your friends? Why?\n→ Yes. Because the training environment is professional, well-structured, and provides hands-on experience that few other places offer.\nSuggestions \u0026amp; Expectations It would be great to have more group-based mini projects for additional practical experience. If possible, I would like to continue in a more advanced program such as the AWS Internship or Cloud Engineer Trainee track. I hope to see more deep-dive content related to cloud security and AI applications in system monitoring. "
},
{
	"uri": "https://phuong721.github.io/learning-aws/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://phuong721.github.io/learning-aws/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]